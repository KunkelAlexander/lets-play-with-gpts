{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1d5813",
   "metadata": {},
   "source": [
    "# Building a GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d73507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-02 17:07:07--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8000::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2026-01-02 17:07:07 (8.94 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d1b200",
   "metadata": {},
   "source": [
    "## 1. Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40525507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008515 M parameters\n",
      "step 0: train loss 4.3340, val loss 4.3339\n",
      "step 100: train loss 3.0835, val loss 3.0942\n",
      "step 200: train loss 2.7372, val loss 2.7498\n",
      "step 300: train loss 2.6258, val loss 2.6455\n",
      "step 400: train loss 2.5821, val loss 2.5955\n",
      "step 500: train loss 2.5468, val loss 2.5685\n",
      "step 600: train loss 2.5298, val loss 2.5464\n",
      "step 700: train loss 2.5060, val loss 2.5276\n",
      "step 800: train loss 2.5092, val loss 2.5206\n",
      "step 900: train loss 2.4980, val loss 2.5208\n",
      "step 1000: train loss 2.4950, val loss 2.5153\n",
      "step 1100: train loss 2.4898, val loss 2.5173\n",
      "step 1200: train loss 2.4936, val loss 2.5101\n",
      "step 1300: train loss 2.4846, val loss 2.5101\n",
      "step 1400: train loss 2.4810, val loss 2.5048\n",
      "step 1500: train loss 2.4819, val loss 2.5011\n",
      "step 1600: train loss 2.4768, val loss 2.4991\n",
      "step 1700: train loss 2.4677, val loss 2.5001\n",
      "step 1800: train loss 2.4798, val loss 2.4976\n",
      "step 1900: train loss 2.4758, val loss 2.4977\n",
      "step 2000: train loss 2.4730, val loss 2.4947\n",
      "step 2100: train loss 2.4685, val loss 2.4916\n",
      "step 2200: train loss 2.4711, val loss 2.4969\n",
      "step 2300: train loss 2.4693, val loss 2.4988\n",
      "step 2400: train loss 2.4641, val loss 2.4881\n",
      "step 2500: train loss 2.4645, val loss 2.4935\n",
      "step 2600: train loss 2.4688, val loss 2.4951\n",
      "step 2700: train loss 2.4706, val loss 2.4878\n",
      "step 2800: train loss 2.4653, val loss 2.5002\n",
      "step 2900: train loss 2.4644, val loss 2.4928\n",
      "step 3000: train loss 2.4712, val loss 2.5041\n",
      "step 3100: train loss 2.4619, val loss 2.4887\n",
      "step 3200: train loss 2.4651, val loss 2.4955\n",
      "step 3300: train loss 2.4642, val loss 2.4914\n",
      "step 3400: train loss 2.4637, val loss 2.4898\n",
      "step 3500: train loss 2.4684, val loss 2.4898\n",
      "step 3600: train loss 2.4682, val loss 2.5009\n",
      "step 3700: train loss 2.4695, val loss 2.4881\n",
      "step 3800: train loss 2.4724, val loss 2.4923\n",
      "step 3900: train loss 2.4668, val loss 2.4862\n",
      "step 4000: train loss 2.4586, val loss 2.4910\n",
      "step 4100: train loss 2.4587, val loss 2.4992\n",
      "step 4200: train loss 2.4656, val loss 2.4875\n",
      "step 4300: train loss 2.4603, val loss 2.4930\n",
      "step 4400: train loss 2.4583, val loss 2.4865\n",
      "step 4500: train loss 2.4602, val loss 2.4906\n",
      "step 4600: train loss 2.4591, val loss 2.4920\n",
      "step 4700: train loss 2.4581, val loss 2.4848\n",
      "step 4800: train loss 2.4644, val loss 2.4887\n",
      "step 4900: train loss 2.4642, val loss 2.4883\n",
      "step 4999: train loss 2.4610, val loss 2.4902\n",
      "\n",
      "Titore, all o I ders domandavalo pr itheewhimousorendouere t che tofo ixes!--n we! olio tifouthand nd athito cameralourgallain My thierinelle atherer, ave re\n",
      "Hol tainchey 'sin, yoratheed paprour y to fome;\n",
      "\n",
      "Thactha e savatwitad,\n",
      "I ENRD\n",
      "Fical hel ly o u fe,\n",
      "Wirune s did at,\n",
      "\n",
      "Winthave helen pe will y bet o lduranewomote!\n",
      "I owingig o I tthiny be ou rs t thiseaseat ss Resth gherct mulenigususunt pincleeacly berd my s.\n",
      "\n",
      "Theronge lay t wigm ile, t tasha houmy ded, o?\n",
      "YIIARat:\n",
      "FFan t mondene sencan\n",
      "Asil my thid\n",
      "\n",
      "\n",
      "ct angempin.\n",
      "\n",
      "Her't s a ayesif tly are, thor tho atitewias t ove ynk' h llal wer inod fan's\n",
      "wistheent\n",
      "Th thid oles we f bowird stsoothisuthofoth atha off?\n",
      "METI f is s; foraneakerseallindepegarlonoato, t\n",
      "S:\n",
      "Theveichat st odin arent mow menesupiglak m g beemof w ron'lvee d they VIn.\n",
      "Whe 's knd iser, s witwnde\n",
      "YO:\n",
      "T:\n",
      "BExhefayent Itere is ous ow teanoinor and hane de ts:\n",
      "Toust topefowier ntelllll cat akethmporewityot tigalath nofowin, fe have s\n",
      "WA:\n",
      "Detoro delilous mis s d orin,\n",
      "S:\n",
      "GEThe t te, fifu sus INCASST:\n",
      "Th oruse pe\n",
      "I if y, fourired idd p th ling,\n",
      "Gou CHearive MAparasg thenor mbrte ble leang we lat!\n",
      "Guss w alre.\n",
      "t; awierkit Bu, tlde, llon, nshonirrinthy.\n",
      "Whe ilestithazedothey chernd\n",
      "Alt l st! thans himprbe hire r m, mo cargit joforthert the ssom athabedofrt gomul ovuind y h wit hesanPUEOFr t per\n",
      "\n",
      "Got'anithe toberld, Tht, m. levepomy ins, me ot thteatinksowaitt y ave th loly bricandour e t ce Ifoupee winoveay heg gok f s ms wicadove mitin, CIS:\n",
      "Fa stoumouty, lllin INCAn toupe,\n",
      "Whaborle:\n",
      "Cle atrvicthent. f wat is ure tey\n",
      "Se, theaiery s, INCHene falllpe houale akengratho thes HETt\n",
      "\n",
      "\n",
      "WIIst s nd thounoffose alle las l f pecce.\n",
      "WhaANoung w leir wain Fotemind.\n",
      "Poligagin ndursu br d. tes, f s fe gim ve tor akin'ss s;\n",
      "D:\n",
      "IStino!\n",
      "thif ithave. lofolithaithioupl bo, brincond femef I bugoare s,\n",
      "Yocen'd s, yorichedue.\n",
      "\n",
      "ARARod gst pigey veancklizequson.\n",
      "D y.\n",
      "\n",
      "ICIf fa irt hitut s:\n",
      "TIFof winduthis myofreef s ke. th t CUERO:\n",
      "Twicucezee t by quthimer d g.\n",
      "Beate tho,\n",
      "Wheve'thathale\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = vocab_size\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x          = token_embd\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b14f2c",
   "metadata": {},
   "source": [
    "## 2. Bigram model with positional encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340561a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010433 M parameters\n",
      "step 0: train loss 4.4806, val loss 4.4632\n",
      "step 100: train loss 3.1635, val loss 3.1623\n",
      "step 200: train loss 2.8049, val loss 2.8097\n",
      "step 300: train loss 2.6793, val loss 2.7050\n",
      "step 400: train loss 2.6254, val loss 2.6509\n",
      "step 500: train loss 2.5888, val loss 2.6089\n",
      "step 600: train loss 2.5651, val loss 2.5830\n",
      "step 700: train loss 2.5464, val loss 2.5575\n",
      "step 800: train loss 2.5429, val loss 2.5555\n",
      "step 900: train loss 2.5317, val loss 2.5476\n",
      "step 1000: train loss 2.5251, val loss 2.5436\n",
      "step 1100: train loss 2.5176, val loss 2.5305\n",
      "step 1200: train loss 2.5150, val loss 2.5197\n",
      "step 1300: train loss 2.5030, val loss 2.5246\n",
      "step 1400: train loss 2.5042, val loss 2.5124\n",
      "step 1500: train loss 2.5029, val loss 2.5127\n",
      "step 1600: train loss 2.4964, val loss 2.5111\n",
      "step 1700: train loss 2.4958, val loss 2.5101\n",
      "step 1800: train loss 2.4934, val loss 2.5087\n",
      "step 1900: train loss 2.4910, val loss 2.5093\n",
      "step 2000: train loss 2.4850, val loss 2.5052\n",
      "step 2100: train loss 2.4890, val loss 2.5049\n",
      "step 2200: train loss 2.4886, val loss 2.5054\n",
      "step 2300: train loss 2.4794, val loss 2.5038\n",
      "step 2400: train loss 2.4796, val loss 2.5038\n",
      "step 2500: train loss 2.4791, val loss 2.4984\n",
      "step 2600: train loss 2.4779, val loss 2.4985\n",
      "step 2700: train loss 2.4795, val loss 2.4955\n",
      "step 2800: train loss 2.4728, val loss 2.4958\n",
      "step 2900: train loss 2.4737, val loss 2.4966\n",
      "step 3000: train loss 2.4778, val loss 2.5068\n",
      "step 3100: train loss 2.4735, val loss 2.4956\n",
      "step 3200: train loss 2.4775, val loss 2.4962\n",
      "step 3300: train loss 2.4727, val loss 2.4985\n",
      "step 3400: train loss 2.4803, val loss 2.4985\n",
      "step 3500: train loss 2.4805, val loss 2.4973\n",
      "step 3600: train loss 2.4767, val loss 2.4998\n",
      "step 3700: train loss 2.4753, val loss 2.4851\n",
      "step 3800: train loss 2.4739, val loss 2.4939\n",
      "step 3900: train loss 2.4684, val loss 2.4959\n",
      "step 4000: train loss 2.4650, val loss 2.4967\n",
      "step 4100: train loss 2.4701, val loss 2.4908\n",
      "step 4200: train loss 2.4697, val loss 2.4882\n",
      "step 4300: train loss 2.4702, val loss 2.4962\n",
      "step 4400: train loss 2.4690, val loss 2.4969\n",
      "step 4500: train loss 2.4662, val loss 2.4914\n",
      "step 4600: train loss 2.4670, val loss 2.4870\n",
      "step 4700: train loss 2.4684, val loss 2.4977\n",
      "step 4800: train loss 2.4670, val loss 2.4862\n",
      "step 4900: train loss 2.4702, val loss 2.4894\n",
      "step 4999: train loss 2.4694, val loss 2.4971\n",
      "\n",
      "Tourome wis w\n",
      "Whendilirove! f th cod mu.\n",
      "hin g.\n",
      "HAREORimar gh ncurret ng hibamee yo durakegekiss w the ngr out fus tsoves,-t thoonouspevinds thoiofive, Hanys presche in'th thy y sthathiliee anore ave meim f geno f. mchith 'statug\n",
      "Cous,\n",
      "To Gilin:\n",
      "TES:\n",
      "Th, t thy, giopeatht falosoofeathaithe\n",
      "\n",
      "\n",
      "I thoungleve manothe ind\n",
      "Cofinor, Jur ng, m an aneby ryoremep our Mise whin ow f ciotcou ispllere\n",
      "\n",
      "HUS:\n",
      "gin car Jupotallthy uthy:\n",
      "S:\n",
      "Wif machorbad gerim heth w g.\n",
      "Ayoudsth sue whor idinoly, se, tham her vet\n",
      "EOLAS:\n",
      "ARThystherus riofe:\n",
      "Phathay t pstouis wh s al Hyo h dur cofrnarinome thar, t r mes orouss atheacexis I an f s,\n",
      "An tet wig ctheensemyow anthorat, rt igese lses; joyoulis at we is SAse h tousse\n",
      "Twe, trande,\n",
      "\n",
      "HES:\n",
      "\n",
      "I loy, thachis g velelindl.\n",
      "Bus:\n",
      "\n",
      "T: ow is met: ly lthok t'Thy f metha m.\n",
      "T: t, uls H:\n",
      "LAE oro touplll: thouma I:\n",
      "er pit ooveckioto ook.\n",
      "ARLA:\n",
      "S:\n",
      "Tinckithiowith t heanucuther tlpire rppe hak d tseand hy.\n",
      "AOFFooous werse geamane\n",
      "\n",
      "Bun thu ofr ore h, wan nocouchiveart t oollas th.\n",
      "\n",
      "Fag ath\n",
      "\n",
      "Ay nor beer AUEofomsad h the m ra inol be.\n",
      "PRUEvo aversinthay y, than\n",
      "NARY:\n",
      "\n",
      "Drurtad\n",
      "As mone, allee hinghay booy, ut pier.\n",
      "LI s oro ghetoo cere st, CAUE f u m s, honourricungiound-s angeren bes pry brsprisheresoreshore f t\n",
      "Wer, t-\n",
      "Whon'tt thatoupldulesoft w ale ave th t, thit wallisth nd meaveerefo ee? y r,\n",
      "Gothed ot s\n",
      "INE ltutheatin: hellecl\n",
      "Tndiche f haiou wistiliea t fo lay l otowatole ous\n",
      "WI w FABuchodaove f dior'ds sucout:\n",
      "\n",
      "t t hefoutreag, ct wleammpe ad con haghand ind isithe to pand f Wha held las ang h pry. h athe ma f the ath,\n",
      "IS:\n",
      "K:\n",
      "Thons ssel ply woy,\n",
      "LUS:\n",
      "\n",
      "Who my fe bu th! t ch mmemexan mon qupevers liomar fed anonoYo inthig tin pr amd noff k you wecu nooug hathigillde whige aitoon:\n",
      "G tik mat, mes Whe ldimyomy at waiveanthos woanefell ICa helay th t I h wat:\n",
      "K:\n",
      "\n",
      "ME n. theeik.\n",
      "im ho t ckim\n",
      "He thooupo y. bot co-ave surit.\n",
      "Bu'Bee TINAst Langrin t swe s ffighormesaldslond ay ndeve ndnear. foula ancade ted ELondiserdound beve t tho yonneam su ouiearsithe ghormm l beritold\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e7e7f",
   "metadata": {},
   "source": [
    "## 3. Single self-attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72f3432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022721 M parameters\n",
      "step 0: train loss 4.2104, val loss 4.2112\n",
      "step 100: train loss 3.0323, val loss 3.0592\n",
      "step 200: train loss 2.8618, val loss 2.8798\n",
      "step 300: train loss 2.7799, val loss 2.7993\n",
      "step 400: train loss 2.7349, val loss 2.7378\n",
      "step 500: train loss 2.6847, val loss 2.6834\n",
      "step 600: train loss 2.6242, val loss 2.6307\n",
      "step 700: train loss 2.5742, val loss 2.5813\n",
      "step 800: train loss 2.5220, val loss 2.5225\n",
      "step 900: train loss 2.4832, val loss 2.4967\n",
      "step 1000: train loss 2.4650, val loss 2.4817\n",
      "step 1100: train loss 2.4456, val loss 2.4503\n",
      "step 1200: train loss 2.4321, val loss 2.4392\n",
      "step 1300: train loss 2.4179, val loss 2.4313\n",
      "step 1400: train loss 2.4076, val loss 2.4225\n",
      "step 1500: train loss 2.4012, val loss 2.4204\n",
      "step 1600: train loss 2.4051, val loss 2.4148\n",
      "step 1700: train loss 2.3884, val loss 2.4016\n",
      "step 1800: train loss 2.3876, val loss 2.4063\n",
      "step 1900: train loss 2.3832, val loss 2.3942\n",
      "step 2000: train loss 2.3772, val loss 2.4047\n",
      "step 2100: train loss 2.3805, val loss 2.3997\n",
      "step 2200: train loss 2.3712, val loss 2.3940\n",
      "step 2300: train loss 2.3667, val loss 2.3940\n",
      "step 2400: train loss 2.3624, val loss 2.3829\n",
      "step 2500: train loss 2.3736, val loss 2.3848\n",
      "step 2600: train loss 2.3664, val loss 2.3859\n",
      "step 2700: train loss 2.3653, val loss 2.3779\n",
      "step 2800: train loss 2.3548, val loss 2.3755\n",
      "step 2900: train loss 2.3459, val loss 2.3698\n",
      "step 3000: train loss 2.3588, val loss 2.3747\n",
      "step 3100: train loss 2.3537, val loss 2.3746\n",
      "step 3200: train loss 2.3468, val loss 2.3628\n",
      "step 3300: train loss 2.3461, val loss 2.3720\n",
      "step 3400: train loss 2.3364, val loss 2.3654\n",
      "step 3500: train loss 2.3477, val loss 2.3692\n",
      "step 3600: train loss 2.3468, val loss 2.3747\n",
      "step 3700: train loss 2.3464, val loss 2.3667\n",
      "step 3800: train loss 2.3342, val loss 2.3628\n",
      "step 3900: train loss 2.3288, val loss 2.3662\n",
      "step 4000: train loss 2.3301, val loss 2.3642\n",
      "step 4100: train loss 2.3306, val loss 2.3591\n",
      "step 4200: train loss 2.3288, val loss 2.3614\n",
      "step 4300: train loss 2.3289, val loss 2.3583\n",
      "step 4400: train loss 2.3220, val loss 2.3642\n",
      "step 4500: train loss 2.3223, val loss 2.3616\n",
      "step 4600: train loss 2.3213, val loss 2.3481\n",
      "step 4700: train loss 2.3246, val loss 2.3536\n",
      "step 4800: train loss 2.3204, val loss 2.3459\n",
      "step 4900: train loss 2.3314, val loss 2.3499\n",
      "step 4999: train loss 2.3208, val loss 2.3448\n",
      "\n",
      "Nicut betet, us,\n",
      "YCouth Noucced wietsh the lich yecbor ot asovice soome over som my hacrieghhon dim tonert min bestt wawir-mingoun cowood one geld shur dotith theries you dses to thal mathird this bok tism;\n",
      "I sangindo ber the.\n",
      "\n",
      "HR:one I pe hirth\n",
      "Pi, me peirshat orot hainck, me!\n",
      "\n",
      "AGLoouf tupiut bor tur;\n",
      "Fry;\n",
      "o thor'ld yath\n",
      "But chat,\n",
      "I thait,\n",
      "Teschit ng es tam, fa\n",
      "Pro st\n",
      "I chath mithuld hiverco mino thant her f\n",
      "yof tsh f othaum mus? bo btheu psh peer to lm vimpr.\n",
      "\n",
      "Hodor, lor brene ou nd whad mus myod wor p tene bium ty ser.\n",
      "BOloig--\n",
      "ONF at hort, thee anis Adr: yod dset of thons! O dlo ftet bere nwith's thinllo, makithe the\n",
      "LEASNO:\n",
      "Tow git, ot.\n",
      "Frincon?\n",
      "\n",
      "Carshed ploswou huntl dungu vigoad as chatir hemnand\n",
      "Tour\n",
      "Hadn nocold--di-br om, tllem fo-t ther tame?\n",
      "\n",
      "CHLIUSinlat, on ofraerourth cihnd,\n",
      "Th the gs fo smy ow ther bathtoel my,\n",
      "RO:\n",
      "Be yout agit teness me; Gs atile rier wisall set cang to measis\n",
      "Her'st foroureg, rif briseto,\n",
      "Pes po imn antt alaunkn:\n",
      "The, ay kit sofr tir yef ow cilout ber dedrerre mssu teatr-pohard gs mow a pelenen ss;\n",
      "Thew my bed hin aty itho?\n",
      "\n",
      "Whe hovecughe odo ithistt:\n",
      "ng, bawre,\n",
      "Be\n",
      "Ad fout the want nad her th a hous mot do itrake! Gn athe iscke llk.\n",
      "\n",
      "Bouresare pine thal ne,\n",
      "T:\n",
      "Bleve,\n",
      "Y theathas to\n",
      "ns Pecrobrlif cod cithreve.\n",
      "\n",
      "And ll asen at, ond stheatebret, the forr\n",
      "Whait ge ks, she I GNoug bomears ist sen.\n",
      "LLO:\n",
      "EYLO:\n",
      "f I wond D, Ewe.\n",
      "\n",
      "Toun thes Nof uenawris?Tee be\n",
      "Gso ll, leer sly fromar whe ses come te bul misein alp:at\n",
      "I mbe fre sise myou she ar?\n",
      "\n",
      "Aro nd,\n",
      "Y:\n",
      "To me ld ly Porus?\n",
      "\n",
      "IS:\n",
      "N Ed I Gn's yu.\n",
      "\n",
      "Bulimeprd, whe nowm sow.\n",
      "\n",
      "GRYouth ind, My the our I'tll thing bang aulcal ppe tharveir s cll thiths t the.\n",
      "\n",
      "If yr tewis fat theen bere tol\n",
      "To bre mm, ld manot me th myoverong olther mey pris wite, tofuancklondeabrok.\n",
      "\n",
      "Tyos for, whoung thin pre te werist gand;\n",
      "Mater ther, rorway che su'd-'s whin.\n",
      "\n",
      "Gporod fo s mancor thy ty cer' te lll aven tepl;\n",
      "h wasthen\n",
      "Abou pango won brakinmy, nid tis lwhile.\n",
      "\n",
      "Whe,\n",
      "Paler:\n",
      "Cwato spatwhise\n",
      "GOPRULUThow frod by,\n",
      "A from h\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.sa_head = Head(n_embd)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.sa_head(x) # (B, T, C)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca8e2b",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76144f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022721 M parameters\n",
      "step 0: train loss 4.2319, val loss 4.2284\n",
      "step 100: train loss 2.9697, val loss 2.9860\n",
      "step 200: train loss 2.7325, val loss 2.7445\n",
      "step 300: train loss 2.6457, val loss 2.6642\n",
      "step 400: train loss 2.6017, val loss 2.6090\n",
      "step 500: train loss 2.5735, val loss 2.5722\n",
      "step 600: train loss 2.5301, val loss 2.5347\n",
      "step 700: train loss 2.5001, val loss 2.5106\n",
      "step 800: train loss 2.4662, val loss 2.4663\n",
      "step 900: train loss 2.4334, val loss 2.4444\n",
      "step 1000: train loss 2.4144, val loss 2.4287\n",
      "step 1100: train loss 2.3892, val loss 2.3841\n",
      "step 1200: train loss 2.3672, val loss 2.3707\n",
      "step 1300: train loss 2.3490, val loss 2.3565\n",
      "step 1400: train loss 2.3306, val loss 2.3360\n",
      "step 1500: train loss 2.3205, val loss 2.3332\n",
      "step 1600: train loss 2.3140, val loss 2.3170\n",
      "step 1700: train loss 2.2920, val loss 2.3016\n",
      "step 1800: train loss 2.2812, val loss 2.3002\n",
      "step 1900: train loss 2.2710, val loss 2.2814\n",
      "step 2000: train loss 2.2548, val loss 2.2888\n",
      "step 2100: train loss 2.2495, val loss 2.2755\n",
      "step 2200: train loss 2.2413, val loss 2.2644\n",
      "step 2300: train loss 2.2325, val loss 2.2646\n",
      "step 2400: train loss 2.2269, val loss 2.2536\n",
      "step 2500: train loss 2.2288, val loss 2.2473\n",
      "step 2600: train loss 2.2179, val loss 2.2460\n",
      "step 2700: train loss 2.2096, val loss 2.2312\n",
      "step 2800: train loss 2.1966, val loss 2.2233\n",
      "step 2900: train loss 2.1844, val loss 2.2204\n",
      "step 3000: train loss 2.1888, val loss 2.2184\n",
      "step 3100: train loss 2.1829, val loss 2.2167\n",
      "step 3200: train loss 2.1718, val loss 2.2005\n",
      "step 3300: train loss 2.1684, val loss 2.2050\n",
      "step 3400: train loss 2.1574, val loss 2.1985\n",
      "step 3500: train loss 2.1646, val loss 2.1987\n",
      "step 3600: train loss 2.1607, val loss 2.2061\n",
      "step 3700: train loss 2.1592, val loss 2.1982\n",
      "step 3800: train loss 2.1409, val loss 2.1911\n",
      "step 3900: train loss 2.1337, val loss 2.1935\n",
      "step 4000: train loss 2.1326, val loss 2.1886\n",
      "step 4100: train loss 2.1307, val loss 2.1818\n",
      "step 4200: train loss 2.1209, val loss 2.1830\n",
      "step 4300: train loss 2.1188, val loss 2.1753\n",
      "step 4400: train loss 2.1123, val loss 2.1789\n",
      "step 4500: train loss 2.1089, val loss 2.1720\n",
      "step 4600: train loss 2.1089, val loss 2.1669\n",
      "step 4700: train loss 2.1077, val loss 2.1626\n",
      "step 4800: train loss 2.1035, val loss 2.1589\n",
      "step 4900: train loss 2.1109, val loss 2.1638\n",
      "step 4999: train loss 2.0942, val loss 2.1530\n",
      "\n",
      "Sicke be that some frold\n",
      "I ace done shath plich yeers to masove sorto,\n",
      "Ford yof fay hat yer\n",
      "Ach dis tonlemush lo sto wawir-nth tin comeon the gald shus dotey therries you dres to dram make ownt is bok tishe!\n",
      "Nor apend brite, is wauone then him to is me plirshaty, Want,\n",
      "WARPUORKK: Gow Rom, biof bor tho;\n",
      "Fry mot cond thy My\n",
      "BELDrIN:\n",
      "'I d sio, welch the Ie wiak, an\n",
      "Or ast\n",
      "Icr, the.\n",
      "\n",
      "PuRILANY BOCE to Masnot her fay forsh froth hof a tobo bellukend porints lo vispr.\n",
      "\n",
      "Hodowerld shat hink not do thus my by trop trepl iumpendse,\n",
      "Nod tie-s\n",
      "Once the tou tree ance And dyid dy But you Is teart.\n",
      "\n",
      "That fre neard' mont, Poold\n",
      "ABUSTEO:\n",
      "Yow the mach git, of.\n",
      "Fornconece\n",
      "Frof douldss low stlly u bring a tas chatir hempay,\n",
      "Tho chadn nocold thish to dowllem fort ther.\n",
      "\n",
      "Ifll comgorinlay, on to antou thad have shomus goct I'm: of ther batht-\n",
      "To with rowell itca\n",
      "Falle pst met you tile ried willll sel cantwich tast sartout To sore kirs\n",
      "Ast sean's thee hime a fth waunknot,\n",
      "I's ch liks an tir yof ow cilound'd\n",
      "be dremre yss--Peatrupth arcis moona peeend. Thar tow MIines him any lomu?\n",
      "\n",
      "Why Os ponele! Howith sit.\n",
      "\n",
      "Jus awne, Blo blll tut shwash nad her trou hous both of\n",
      "Icant! Gnobllid shire loper; themare pine thalle.\n",
      "\n",
      "CENTIO:\n",
      "To drer,\n",
      "Rast\n",
      "Thes Puch\n",
      "Thous comedid ce, Frinke lus\n",
      "And at, ond sond beir thave shorr\n",
      "Whave gee farst tell owhe\n",
      "Angays is ife hin lord: forf mot\n",
      "LArD, EDI I'l boble sirt: thiminightee be\n",
      "For a by ner sly from and to me come tel we mis in alpoad\n",
      "I llitf:\n",
      "Beare scand hempe?\n",
      "\n",
      "Ero nip dare in to the Pores?\n",
      "\n",
      "I HARKE:\n",
      "When's yound plieds, row be prm rre.\n",
      "\n",
      "LIOA:\n",
      "I ind, Wherem Paroth lust ing broveaulf loppeat armiishe clorth tos deact.\n",
      "\n",
      "If you ear's awelldeef crear\n",
      "Thane tem. Ta dor Woa thish mooverows olt\n",
      "Say:\n",
      "Howrls with, tof anck. MISabto gitry.\n",
      "A:\n",
      "Thou yfor thy with to dusist; and kin's ct sut roplay cord you-'s wom Sir, I of buts mance\n",
      "Ashat yice ' twill tamen to the;\n",
      "\n",
      "SSY:\n",
      "Ser ou paimy wor draking their tis lord wess?\n",
      "\n",
      "LAPUTING:\n",
      "You and Cwond to swith bus ford bvy to you b\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.sa_heads = MultiHeadAttention(n_head, n_embd // n_head)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.sa_heads(x) # (B, T, C)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c727121",
   "metadata": {},
   "source": [
    "## 5. Adding Feed-Forward Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ab46c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bef7eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026881 M parameters\n",
      "step 0: train loss 4.1892, val loss 4.1898\n",
      "step 100: train loss 2.9667, val loss 2.9977\n",
      "step 200: train loss 2.7191, val loss 2.7361\n",
      "step 300: train loss 2.6334, val loss 2.6477\n",
      "step 400: train loss 2.5871, val loss 2.5963\n",
      "step 500: train loss 2.5438, val loss 2.5422\n",
      "step 600: train loss 2.5135, val loss 2.5134\n",
      "step 700: train loss 2.4809, val loss 2.4861\n",
      "step 800: train loss 2.4581, val loss 2.4707\n",
      "step 900: train loss 2.4290, val loss 2.4333\n",
      "step 1000: train loss 2.4147, val loss 2.4108\n",
      "step 1100: train loss 2.3801, val loss 2.3951\n",
      "step 1200: train loss 2.3699, val loss 2.3635\n",
      "step 1300: train loss 2.3446, val loss 2.3507\n",
      "step 1400: train loss 2.3241, val loss 2.3410\n",
      "step 1500: train loss 2.3159, val loss 2.3270\n",
      "step 1600: train loss 2.2974, val loss 2.3027\n",
      "step 1700: train loss 2.2848, val loss 2.3037\n",
      "step 1800: train loss 2.2687, val loss 2.2905\n",
      "step 1900: train loss 2.2630, val loss 2.2748\n",
      "step 2000: train loss 2.2474, val loss 2.2701\n",
      "step 2100: train loss 2.2369, val loss 2.2578\n",
      "step 2200: train loss 2.2312, val loss 2.2573\n",
      "step 2300: train loss 2.2171, val loss 2.2398\n",
      "step 2400: train loss 2.2043, val loss 2.2290\n",
      "step 2500: train loss 2.2001, val loss 2.2235\n",
      "step 2600: train loss 2.1915, val loss 2.2232\n",
      "step 2700: train loss 2.1787, val loss 2.2108\n",
      "step 2800: train loss 2.1710, val loss 2.2158\n",
      "step 2900: train loss 2.1634, val loss 2.1969\n",
      "step 3000: train loss 2.1577, val loss 2.1948\n",
      "step 3100: train loss 2.1452, val loss 2.1942\n",
      "step 3200: train loss 2.1445, val loss 2.1868\n",
      "step 3300: train loss 2.1423, val loss 2.1779\n",
      "step 3400: train loss 2.1377, val loss 2.1804\n",
      "step 3500: train loss 2.1264, val loss 2.1600\n",
      "step 3600: train loss 2.1197, val loss 2.1633\n",
      "step 3700: train loss 2.1104, val loss 2.1577\n",
      "step 3800: train loss 2.0989, val loss 2.1619\n",
      "step 3900: train loss 2.0978, val loss 2.1477\n",
      "step 4000: train loss 2.0899, val loss 2.1422\n",
      "step 4100: train loss 2.0807, val loss 2.1416\n",
      "step 4200: train loss 2.0784, val loss 2.1413\n",
      "step 4300: train loss 2.0690, val loss 2.1314\n",
      "step 4400: train loss 2.0663, val loss 2.1306\n",
      "step 4500: train loss 2.0656, val loss 2.1285\n",
      "step 4600: train loss 2.0624, val loss 2.1215\n",
      "step 4700: train loss 2.0552, val loss 2.1229\n",
      "step 4800: train loss 2.0475, val loss 2.1082\n",
      "step 4900: train loss 2.0449, val loss 2.1174\n",
      "step 4999: train loss 2.0425, val loss 2.1129\n",
      "\n",
      "The shat'l liveny crocked and Kivesiome,\n",
      "For somf your onem\n",
      "To prien onertusion: sto wawir-ning.\n",
      "OLAL:\n",
      "Bailne grads our doter thersies you dred ivund frover'own are bek tisme!\n",
      "NUSENS:\n",
      "No ARETH:\n",
      "Tuardone wern he that sains firshis of tin,\n",
      "LANIUORTHice avy tourion ber thours,\n",
      "Our show'e; you your crome, your of weshal, nowe!\n",
      "On'l fa\n",
      "On. Wipsir, theriende.\n",
      "\n",
      "KE BOLE ORTH:\n",
      "Sot he cady fits vitetincocius? bor then pers,\n",
      "Theas lieviine. Coodore'ld shat hin he his ins!\n",
      "With'd hip my mel iun treser.\n",
      "BOLEi:\n",
      "Lord; a perreat sok and toornd in deat of thens! O drihf, thoure nestir mos, leat my ithe ish\n",
      "LEdade mach git, of, Frincon?\n",
      "\n",
      "Carpher plass lielstled ut avined her chat hicemcann\n",
      "Tood cadn, woold-peith tointellem!\n",
      "\n",
      "'t ther untrutsy ightinlay, on of and brad cis Buts! AR:\n",
      "Jo de smiood with bathtee butticerowe you ca\n",
      "Fall hess my mys\n",
      "BRiTh ried wipainss therng to marsiks.\n",
      "\n",
      "ROSI:\n",
      "Thore king ise seven a would on a froplaun my cre's chall sof OriWhis now ciloun 'te deeremre masurie, my that cis my hat'edenen swarl wered\n",
      "Whinh forey ithournd tresip telelornyiod's treng,\n",
      "AUNThat lood four his wash nad her the or and oand theriked un and mincere leagl;\n",
      "Aresare pire thalle dom: woe then lerae\n",
      "fort and unce\n",
      "Calif comedinh joking and: thing at,\n",
      "And srear ibrothat, Oeker\n",
      "Wrainnge, fals.\n",
      "\n",
      "I'llow as hears is iseneriel plater\n",
      "forch\n",
      "LEONI:\n",
      "Ell of but gusirffournair, saye be\n",
      "FiRe\n",
      "FIUMMEN RIIVHARD:\n",
      "INGARIIO:\n",
      "The te bur misein alp: Whawelinfer eare mvand he Fell brozen,\n",
      "You sir is the do frient my pradine,\n",
      "Thally.\n",
      "\n",
      "I mostrres be prm and.\n",
      "\n",
      "GLOUCENTES:\n",
      "Buthrem his but us ding bare aulcal ppeating gir incly theems. beld.\n",
      "\n",
      "KING ENewilif withre ficen to mane ard.\n",
      "\n",
      "PLETRBUS:\n",
      "On thom ovarows olthery kingris with,\n",
      "Tof anciles sabropmithy.\n",
      "Afor, whing bary.\n",
      "\n",
      "KANGRARGAE:\n",
      "On Monk ther ther, rop ay cord you-'m inme.\n",
      "\n",
      "Rpiaodlef gre, a hermy thiced' teill tamen to lord?\n",
      "\n",
      "Sokente ourp coo for may lamy, nifen,\n",
      "On!\n",
      "Thresser of your have of Ranwy der cropouthes from by hippy my solkess omint sthore;\n",
      "Thre to co\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.sa_heads = MultiHeadAttention(n_head, n_embd // n_head)\n",
    "\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.sa_heads(x) # (B, T, C)\n",
    "        x          = self.ffwd(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23c285",
   "metadata": {},
   "source": [
    "## 6. Repeat in blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e0b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059777 M parameters\n",
      "step 0: train loss 4.1798, val loss 4.1788\n",
      "step 100: train loss 3.3162, val loss 3.3529\n",
      "step 200: train loss 3.3100, val loss 3.3587\n",
      "step 300: train loss 3.2665, val loss 3.2892\n",
      "step 400: train loss 3.1412, val loss 3.1535\n",
      "step 500: train loss 3.1087, val loss 3.0994\n",
      "step 600: train loss 3.0788, val loss 3.0750\n",
      "step 700: train loss 2.9811, val loss 2.9787\n",
      "step 800: train loss 2.9058, val loss 2.8864\n",
      "step 900: train loss 2.8358, val loss 2.8208\n",
      "step 1000: train loss 2.7218, val loss 2.7191\n",
      "step 1100: train loss 2.6741, val loss 2.6617\n",
      "step 1200: train loss 2.6260, val loss 2.6276\n",
      "step 1300: train loss 2.5968, val loss 2.5960\n",
      "step 1400: train loss 2.5719, val loss 2.5805\n",
      "step 1500: train loss 2.5399, val loss 2.5307\n",
      "step 1600: train loss 2.5331, val loss 2.5300\n",
      "step 1700: train loss 2.5056, val loss 2.5115\n",
      "step 1800: train loss 2.4904, val loss 2.4856\n",
      "step 1900: train loss 2.4726, val loss 2.4759\n",
      "step 2000: train loss 2.4691, val loss 2.4682\n",
      "step 2100: train loss 2.4595, val loss 2.4622\n",
      "step 2200: train loss 2.4573, val loss 2.4489\n",
      "step 2300: train loss 2.4312, val loss 2.4282\n",
      "step 2400: train loss 2.4077, val loss 2.4244\n",
      "step 2500: train loss 2.4231, val loss 2.4292\n",
      "step 2600: train loss 2.4059, val loss 2.4160\n",
      "step 2700: train loss 2.3994, val loss 2.4097\n",
      "step 2800: train loss 2.3932, val loss 2.3930\n",
      "step 2900: train loss 2.3815, val loss 2.3962\n",
      "step 3000: train loss 2.3624, val loss 2.3740\n",
      "step 3100: train loss 2.3633, val loss 2.3755\n",
      "step 3200: train loss 2.3424, val loss 2.3508\n",
      "step 3300: train loss 2.3315, val loss 2.3438\n",
      "step 3400: train loss 2.3119, val loss 2.3359\n",
      "step 3500: train loss 2.3133, val loss 2.3211\n",
      "step 3600: train loss 2.2994, val loss 2.3280\n",
      "step 3700: train loss 2.2918, val loss 2.3114\n",
      "step 3800: train loss 2.2992, val loss 2.3090\n",
      "step 3900: train loss 2.2824, val loss 2.2985\n",
      "step 4000: train loss 2.2688, val loss 2.2837\n",
      "step 4100: train loss 2.2739, val loss 2.2921\n",
      "step 4200: train loss 2.2516, val loss 2.2603\n",
      "step 4300: train loss 2.2456, val loss 2.2664\n",
      "step 4400: train loss 2.2425, val loss 2.2686\n",
      "step 4500: train loss 2.2329, val loss 2.2486\n",
      "step 4600: train loss 2.2157, val loss 2.2422\n",
      "step 4700: train loss 2.2361, val loss 2.2457\n",
      "step 4800: train loss 2.2047, val loss 2.2233\n",
      "step 4900: train loss 2.1954, val loss 2.2255\n",
      "step 4999: train loss 2.1888, val loss 2.2177\n",
      "\n",
      "\n",
      "E:\n",
      "Mefr u jower to therlay lean.\n",
      "\n",
      "NUJULHIA:\n",
      "Hot, hand lost she hent\n",
      "but and tha' sap to san: ou that I felom.\n",
      "\n",
      "BUGMKOPT:\n",
      "HTou withe pesyom ferlc see! lyouter,\n",
      "Se my forp in spumos felal Coithe of boly seets, bed thall to my that thry to pilte;\n",
      "Theree slour mesom I moy ston sitk\n",
      "So far lele senees beas uchic tith,\n",
      "Horn the sair, I gef till hent tat on tha heil\n",
      "'As firl bob?\n",
      "\n",
      "Fe somingt; At liirar dros dusat, d the neear; neg?\n",
      "\n",
      "CORIDDUASABRIN:\n",
      "Bu thif this sunt you to' shore le dive in helfw cicht\n",
      "Rote ane and your wallate jilles, mems me abid\n",
      "t the linivent hy digh mory.\n",
      "\n",
      "KING VEMLRCECTIO:\n",
      "Viik und gayt at thillea.\n",
      "\n",
      "LPIIONGCINGUS:\n",
      "And Du thers, I le shill me thert thes tou hy nou con'd thou lepef:\n",
      "I'l!\n",
      "\n",
      "Yur no Rrave, I wat thee woudd be non bathish tour ta mare tcee woul fun to at oulsm:\n",
      "Mit lakar\n",
      "ant thou raorts I tha with ar Rumt thea ale, tit you mas lexou foor gin: I we;\n",
      "Tondo I hid dlothe of my band\n",
      "And natn tibs Nus in mowce to pight of withe lan, meigsh of\n",
      "gomer thak, tha mairog, the nit will hen youl nik hid pere'd Wlidinged.\n",
      "\n",
      "QULENLIO:\n",
      "I':\n",
      "And citcn.\n",
      "\n",
      "\n",
      "INLAN BBERDSY YCefor.\n",
      "\n",
      "CEN:\n",
      "Nr no now erst pliy, ea by civered\n",
      "Tothe hean the that thy, lllou. CENTRWtim icoous my nollak;\n",
      "And a moyw mn and.\n",
      "\n",
      "KA EMWVEMMRO:\n",
      "Sefe nor Enaw in poveming fa\n",
      "As howons now to\n",
      "B'ims thou he my smouren at tou renafs\n",
      "And sitricoist sens that ey\n",
      "oi wee stpey I thourcol fnor; wut no abs mert will with ar,\n",
      "Mist they ustey thou slayel tis a herio\n",
      "End Ctanw catcloing nond\n",
      "Id rovey cron tis wer; thy grioy tro pend tash' ofon Wsreaol, bimy\n",
      "cage manviund wholme: gawr doul ree pevun fens ust an, is mort ip imom of booll to mild,\n",
      "Who Rof ciny fon dea\n",
      "te in puvoan Masule.\n",
      "\n",
      "As gim, tit thisk mirer ther ath me my geeencir a kleac to mise.\n",
      "\n",
      "LANOREUL:\n",
      "I, en gangan al tower fmins you,\n",
      "Wore hep I thich you sun wo day:\n",
      "Mror Dor thing in hit coule onh at o;---\n",
      "\n",
      "JuoUMARILB:\n",
      "I pit non I, and a ufe mak re? af for wuth frnone witer: whins in hear untuled\n",
      "Lat the ton thanl gent thould, swar;\n",
      "Tel Rathe lomd t\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "        )\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513bb78a",
   "metadata": {},
   "source": [
    "## 7. Introduce residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daa88d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.084737 M parameters\n",
      "step 0: train loss 4.5349, val loss 4.5202\n",
      "step 100: train loss 2.6950, val loss 2.7011\n",
      "step 200: train loss 2.5318, val loss 2.5242\n",
      "step 300: train loss 2.4481, val loss 2.4419\n",
      "step 400: train loss 2.3868, val loss 2.3886\n",
      "step 500: train loss 2.3467, val loss 2.3377\n",
      "step 600: train loss 2.2987, val loss 2.3040\n",
      "step 700: train loss 2.2525, val loss 2.2777\n",
      "step 800: train loss 2.2299, val loss 2.2504\n",
      "step 900: train loss 2.1834, val loss 2.2027\n",
      "step 1000: train loss 2.1592, val loss 2.1902\n",
      "step 1100: train loss 2.1403, val loss 2.1627\n",
      "step 1200: train loss 2.1066, val loss 2.1492\n",
      "step 1300: train loss 2.0833, val loss 2.1240\n",
      "step 1400: train loss 2.0737, val loss 2.1272\n",
      "step 1500: train loss 2.0634, val loss 2.1045\n",
      "step 1600: train loss 2.0309, val loss 2.0970\n",
      "step 1700: train loss 2.0202, val loss 2.0873\n",
      "step 1800: train loss 2.0046, val loss 2.0637\n",
      "step 1900: train loss 1.9954, val loss 2.0574\n",
      "step 2000: train loss 1.9775, val loss 2.0362\n",
      "step 2100: train loss 1.9587, val loss 2.0335\n",
      "step 2200: train loss 1.9615, val loss 2.0338\n",
      "step 2300: train loss 1.9488, val loss 2.0346\n",
      "step 2400: train loss 1.9297, val loss 2.0281\n",
      "step 2500: train loss 1.9287, val loss 2.0289\n",
      "step 2600: train loss 1.9290, val loss 2.0050\n",
      "step 2700: train loss 1.9061, val loss 2.0128\n",
      "step 2800: train loss 1.9029, val loss 2.0081\n",
      "step 2900: train loss 1.8841, val loss 1.9952\n",
      "step 3000: train loss 1.8823, val loss 1.9791\n",
      "step 3100: train loss 1.8633, val loss 1.9856\n",
      "step 3200: train loss 1.8677, val loss 1.9889\n",
      "step 3300: train loss 1.8577, val loss 1.9845\n",
      "step 3400: train loss 1.8551, val loss 1.9792\n",
      "step 3500: train loss 1.8623, val loss 1.9729\n",
      "step 3600: train loss 1.8335, val loss 1.9651\n",
      "step 3700: train loss 1.8325, val loss 1.9647\n",
      "step 3800: train loss 1.8265, val loss 1.9599\n",
      "step 3900: train loss 1.8277, val loss 1.9557\n",
      "step 4000: train loss 1.8307, val loss 1.9484\n",
      "step 4100: train loss 1.8184, val loss 1.9465\n",
      "step 4200: train loss 1.8139, val loss 1.9402\n",
      "step 4300: train loss 1.7972, val loss 1.9473\n",
      "step 4400: train loss 1.7970, val loss 1.9345\n",
      "step 4500: train loss 1.7906, val loss 1.9290\n",
      "step 4600: train loss 1.7893, val loss 1.9383\n",
      "step 4700: train loss 1.7876, val loss 1.9330\n",
      "step 4800: train loss 1.7709, val loss 1.9241\n",
      "step 4900: train loss 1.7914, val loss 1.9296\n",
      "step 4999: train loss 1.7736, val loss 1.9217\n",
      "\n",
      "DUTERBY:\n",
      "Cometady prost, and thy love thrand speak,\n",
      "Lay.\n",
      "\n",
      "ISLANA:\n",
      "Selvaight.\n",
      "Shall sity so far from our parfe.\n",
      "\n",
      "CLARD YORK:\n",
      "Hown Citize! Vevain; thee holt takeng.\n",
      "\n",
      "KLARD:\n",
      "'A:\n",
      "Yet a gart me a mind high prisiarvess day;\n",
      "But the news that'st the that\n",
      "with magh that this succeds:\n",
      "Some tore of divesing thy love.\n",
      "\n",
      "ROMEO:\n",
      "The stat what's the igner, my shou he dut the lie,\n",
      "And herenge more of to theirs from's stilven igattent, of hearth the for beht, crood of that hile stall of thrraw-heseep'd him; here'd mench,\n",
      "Breed wors; Is not raven, and on, evengent they madamished.\n",
      "\n",
      "LADY:\n",
      "I mace crould quart, thou smy didiln;\n",
      "foant thou rast that; what it we you mess,\n",
      "Besicion Mogh, I shall not gones the sups of it?\n",
      "\n",
      "ARGARET:\n",
      "Sefer, whilk nate tible shall mance;\n",
      "Your ravor with: laid me gabood good whicks that all.\n",
      "My grong, which would lond mack,\n",
      "And in live's do'd friever sIIIul,- Vonce.\n",
      "\n",
      "\n",
      "KING RICHARD IIII:\n",
      "Sig, crrow too have here, wither by caver'd. What hear In time,\n",
      "Ray, lord of Edward madcon; for not all my carmoy-sentate.\n",
      "\n",
      "KINCABUTES:\n",
      "I'Gf that that the porr'd.\n",
      "\n",
      "FEDAR:\n",
      "I that my evor'd swilveed, my lard.\n",
      "herse ruirent is mate: Dicors. York'st form\n",
      "ois ceman that thou, thou bray unting by me those a bacome,\n",
      "Mistores our pervits, lay lievist hears\n",
      "pittitusia, thlein;\n",
      "Pif med fover thon time to mest of thereo peace\n",
      "As him on his gove in yecall! & vise, wholm, lover do'd deep:\n",
      "That sham steap, is most in imbed hop by lame\n",
      "This s;\n",
      "Metizes your souchter. Frust In maurs.\n",
      "\n",
      "SAR:\n",
      "O, this with mines theirst she maken their baks.\n",
      "\n",
      "MENCAPH:\n",
      "More molike sofe what that it weref yor you,\n",
      "But a may buljia you suckingday,\n",
      "My lover then tine time use on tames;\n",
      "Shall out pocclarve, my sofe. God, ufter!\n",
      "\n",
      "KEMINGH:\n",
      "Then do scond, my her disswird, and bented\n",
      "Lay them what over? my tebled, swer;\n",
      "Tell they lamer if Fring; fliegness way.\n",
      "Let their.\n",
      "\n",
      "SICHARD II:\n",
      "Sis their, is noly cherese hour claugs's anothattick,\n",
      "Stand wolsure or of deve! Hext chal ouch.\n",
      "Siruc it ving barg; faw lotes; and that peit fo\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x) + x  # We fork off here\n",
    "        x = self.ffwd(x) + x\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "        )\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402fac4a",
   "metadata": {},
   "source": [
    "## 8. Increase inner dimension in feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbf272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.159041 M parameters\n",
      "step 0: train loss 4.5632, val loss 4.5501\n",
      "step 100: train loss 2.6242, val loss 2.6368\n",
      "step 200: train loss 2.5005, val loss 2.5124\n",
      "step 300: train loss 2.3973, val loss 2.4213\n",
      "step 400: train loss 2.3344, val loss 2.3598\n",
      "step 500: train loss 2.2989, val loss 2.3104\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x) + x  # We fork off here\n",
    "        x = self.ffwd(x) + x\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "        )\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7819873",
   "metadata": {},
   "source": [
    "## 9. LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f430cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.060673 M parameters\n",
      "step 0: train loss 4.3583, val loss 4.3437\n",
      "step 100: train loss 2.8025, val loss 2.8085\n",
      "step 200: train loss 2.5875, val loss 2.5920\n",
      "step 300: train loss 2.5066, val loss 2.5041\n",
      "step 400: train loss 2.4472, val loss 2.4586\n",
      "step 500: train loss 2.4045, val loss 2.4016\n",
      "step 600: train loss 2.3604, val loss 2.3588\n",
      "step 700: train loss 2.3098, val loss 2.3202\n",
      "step 800: train loss 2.2878, val loss 2.2833\n",
      "step 900: train loss 2.2519, val loss 2.2654\n",
      "step 1000: train loss 2.2132, val loss 2.2304\n",
      "step 1100: train loss 2.2026, val loss 2.2219\n",
      "step 1200: train loss 2.1700, val loss 2.1874\n",
      "step 1300: train loss 2.1585, val loss 2.1837\n",
      "step 1400: train loss 2.1327, val loss 2.1663\n",
      "step 1500: train loss 2.1176, val loss 2.1522\n",
      "step 1600: train loss 2.0933, val loss 2.1298\n",
      "step 1700: train loss 2.0747, val loss 2.1238\n",
      "step 1800: train loss 2.0721, val loss 2.1085\n",
      "step 1900: train loss 2.0440, val loss 2.1007\n",
      "step 2000: train loss 2.0370, val loss 2.1018\n",
      "step 2100: train loss 2.0242, val loss 2.0912\n",
      "step 2200: train loss 2.0229, val loss 2.0824\n",
      "step 2300: train loss 2.0057, val loss 2.0625\n",
      "step 2400: train loss 1.9799, val loss 2.0597\n",
      "step 2500: train loss 1.9843, val loss 2.0528\n",
      "step 2600: train loss 1.9719, val loss 2.0541\n",
      "step 2700: train loss 1.9537, val loss 2.0422\n",
      "step 2800: train loss 1.9562, val loss 2.0353\n",
      "step 2900: train loss 1.9511, val loss 2.0297\n",
      "step 3000: train loss 1.9370, val loss 2.0205\n",
      "step 3100: train loss 1.9308, val loss 2.0241\n",
      "step 3200: train loss 1.9113, val loss 2.0084\n",
      "step 3300: train loss 1.9146, val loss 2.0046\n",
      "step 3400: train loss 1.8929, val loss 2.0030\n",
      "step 3500: train loss 1.9018, val loss 1.9983\n",
      "step 3600: train loss 1.8856, val loss 2.0036\n",
      "step 3700: train loss 1.8800, val loss 1.9986\n",
      "step 3800: train loss 1.8901, val loss 2.0038\n",
      "step 3900: train loss 1.8722, val loss 1.9926\n",
      "step 4000: train loss 1.8663, val loss 1.9838\n",
      "step 4100: train loss 1.8691, val loss 1.9835\n",
      "step 4200: train loss 1.8630, val loss 1.9718\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 196\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 196\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 58\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[1;32m     57\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[0;32m---> 58\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     60\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 152\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    150\u001b[0m pos_embd   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;66;03m# (T, c)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m x          \u001b[38;5;241m=\u001b[39m token_embd \u001b[38;5;241m+\u001b[39m pos_embd \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m x          \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m logits     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_head(x) \u001b[38;5;66;03m# (B, T, vocab_size)\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 125\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 125\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 100\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 100\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 85\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m wei \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m C\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;66;03m# (B, T, C) @ (B, C, T) -> (B, T, T)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m wei \u001b[38;5;241m=\u001b[39m wei\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril[:T, :T] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwei\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(wei)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# perform the weighted aggregation of the values\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2140\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1  = nn.LayerNorm(n_embd)\n",
    "        self.ln2  = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))   # We fork off here\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "        )\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9728927c",
   "metadata": {},
   "source": [
    "## 10. Scale it up by stacking blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab316889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=n_head) for _ in range(n_layer)\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        x          = self.ln_f(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67154294",
   "metadata": {},
   "source": [
    "## 11. Enable dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=n_head) for _ in range(n_layer)\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        x          = self.ln_f(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
