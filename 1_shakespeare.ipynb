{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1d5813",
   "metadata": {},
   "source": [
    "# Building a GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d73507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-02 17:07:07--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8000::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2026-01-02 17:07:07 (8.94 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d1b200",
   "metadata": {},
   "source": [
    "## 1. Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40525507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008515 M parameters\n",
      "step 0: train loss 4.3340, val loss 4.3339\n",
      "step 100: train loss 3.0835, val loss 3.0942\n",
      "step 200: train loss 2.7372, val loss 2.7498\n",
      "step 300: train loss 2.6258, val loss 2.6455\n",
      "step 400: train loss 2.5821, val loss 2.5955\n",
      "step 500: train loss 2.5468, val loss 2.5685\n",
      "step 600: train loss 2.5298, val loss 2.5464\n",
      "step 700: train loss 2.5060, val loss 2.5276\n",
      "step 800: train loss 2.5092, val loss 2.5206\n",
      "step 900: train loss 2.4980, val loss 2.5208\n",
      "step 1000: train loss 2.4950, val loss 2.5153\n",
      "step 1100: train loss 2.4898, val loss 2.5173\n",
      "step 1200: train loss 2.4936, val loss 2.5101\n",
      "step 1300: train loss 2.4846, val loss 2.5101\n",
      "step 1400: train loss 2.4810, val loss 2.5048\n",
      "step 1500: train loss 2.4819, val loss 2.5011\n",
      "step 1600: train loss 2.4768, val loss 2.4991\n",
      "step 1700: train loss 2.4677, val loss 2.5001\n",
      "step 1800: train loss 2.4798, val loss 2.4976\n",
      "step 1900: train loss 2.4758, val loss 2.4977\n",
      "step 2000: train loss 2.4730, val loss 2.4947\n",
      "step 2100: train loss 2.4685, val loss 2.4916\n",
      "step 2200: train loss 2.4711, val loss 2.4969\n",
      "step 2300: train loss 2.4693, val loss 2.4988\n",
      "step 2400: train loss 2.4641, val loss 2.4881\n",
      "step 2500: train loss 2.4645, val loss 2.4935\n",
      "step 2600: train loss 2.4688, val loss 2.4951\n",
      "step 2700: train loss 2.4706, val loss 2.4878\n",
      "step 2800: train loss 2.4653, val loss 2.5002\n",
      "step 2900: train loss 2.4644, val loss 2.4928\n",
      "step 3000: train loss 2.4712, val loss 2.5041\n",
      "step 3100: train loss 2.4619, val loss 2.4887\n",
      "step 3200: train loss 2.4651, val loss 2.4955\n",
      "step 3300: train loss 2.4642, val loss 2.4914\n",
      "step 3400: train loss 2.4637, val loss 2.4898\n",
      "step 3500: train loss 2.4684, val loss 2.4898\n",
      "step 3600: train loss 2.4682, val loss 2.5009\n",
      "step 3700: train loss 2.4695, val loss 2.4881\n",
      "step 3800: train loss 2.4724, val loss 2.4923\n",
      "step 3900: train loss 2.4668, val loss 2.4862\n",
      "step 4000: train loss 2.4586, val loss 2.4910\n",
      "step 4100: train loss 2.4587, val loss 2.4992\n",
      "step 4200: train loss 2.4656, val loss 2.4875\n",
      "step 4300: train loss 2.4603, val loss 2.4930\n",
      "step 4400: train loss 2.4583, val loss 2.4865\n",
      "step 4500: train loss 2.4602, val loss 2.4906\n",
      "step 4600: train loss 2.4591, val loss 2.4920\n",
      "step 4700: train loss 2.4581, val loss 2.4848\n",
      "step 4800: train loss 2.4644, val loss 2.4887\n",
      "step 4900: train loss 2.4642, val loss 2.4883\n",
      "step 4999: train loss 2.4610, val loss 2.4902\n",
      "\n",
      "Titore, all o I ders domandavalo pr itheewhimousorendouere t che tofo ixes!--n we! olio tifouthand nd athito cameralourgallain My thierinelle atherer, ave re\n",
      "Hol tainchey 'sin, yoratheed paprour y to fome;\n",
      "\n",
      "Thactha e savatwitad,\n",
      "I ENRD\n",
      "Fical hel ly o u fe,\n",
      "Wirune s did at,\n",
      "\n",
      "Winthave helen pe will y bet o lduranewomote!\n",
      "I owingig o I tthiny be ou rs t thiseaseat ss Resth gherct mulenigususunt pincleeacly berd my s.\n",
      "\n",
      "Theronge lay t wigm ile, t tasha houmy ded, o?\n",
      "YIIARat:\n",
      "FFan t mondene sencan\n",
      "Asil my thid\n",
      "\n",
      "\n",
      "ct angempin.\n",
      "\n",
      "Her't s a ayesif tly are, thor tho atitewias t ove ynk' h llal wer inod fan's\n",
      "wistheent\n",
      "Th thid oles we f bowird stsoothisuthofoth atha off?\n",
      "METI f is s; foraneakerseallindepegarlonoato, t\n",
      "S:\n",
      "Theveichat st odin arent mow menesupiglak m g beemof w ron'lvee d they VIn.\n",
      "Whe 's knd iser, s witwnde\n",
      "YO:\n",
      "T:\n",
      "BExhefayent Itere is ous ow teanoinor and hane de ts:\n",
      "Toust topefowier ntelllll cat akethmporewityot tigalath nofowin, fe have s\n",
      "WA:\n",
      "Detoro delilous mis s d orin,\n",
      "S:\n",
      "GEThe t te, fifu sus INCASST:\n",
      "Th oruse pe\n",
      "I if y, fourired idd p th ling,\n",
      "Gou CHearive MAparasg thenor mbrte ble leang we lat!\n",
      "Guss w alre.\n",
      "t; awierkit Bu, tlde, llon, nshonirrinthy.\n",
      "Whe ilestithazedothey chernd\n",
      "Alt l st! thans himprbe hire r m, mo cargit joforthert the ssom athabedofrt gomul ovuind y h wit hesanPUEOFr t per\n",
      "\n",
      "Got'anithe toberld, Tht, m. levepomy ins, me ot thteatinksowaitt y ave th loly bricandour e t ce Ifoupee winoveay heg gok f s ms wicadove mitin, CIS:\n",
      "Fa stoumouty, lllin INCAn toupe,\n",
      "Whaborle:\n",
      "Cle atrvicthent. f wat is ure tey\n",
      "Se, theaiery s, INCHene falllpe houale akengratho thes HETt\n",
      "\n",
      "\n",
      "WIIst s nd thounoffose alle las l f pecce.\n",
      "WhaANoung w leir wain Fotemind.\n",
      "Poligagin ndursu br d. tes, f s fe gim ve tor akin'ss s;\n",
      "D:\n",
      "IStino!\n",
      "thif ithave. lofolithaithioupl bo, brincond femef I bugoare s,\n",
      "Yocen'd s, yorichedue.\n",
      "\n",
      "ARARod gst pigey veancklizequson.\n",
      "D y.\n",
      "\n",
      "ICIf fa irt hitut s:\n",
      "TIFof winduthis myofreef s ke. th t CUERO:\n",
      "Twicucezee t by quthimer d g.\n",
      "Beate tho,\n",
      "Wheve'thathale\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = vocab_size\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x          = token_embd\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b14f2c",
   "metadata": {},
   "source": [
    "## 2. Bigram model with positional encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340561a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010433 M parameters\n",
      "step 0: train loss 4.4806, val loss 4.4632\n",
      "step 100: train loss 3.1635, val loss 3.1623\n",
      "step 200: train loss 2.8049, val loss 2.8097\n",
      "step 300: train loss 2.6793, val loss 2.7050\n",
      "step 400: train loss 2.6254, val loss 2.6509\n",
      "step 500: train loss 2.5888, val loss 2.6089\n",
      "step 600: train loss 2.5651, val loss 2.5830\n",
      "step 700: train loss 2.5464, val loss 2.5575\n",
      "step 800: train loss 2.5429, val loss 2.5555\n",
      "step 900: train loss 2.5317, val loss 2.5476\n",
      "step 1000: train loss 2.5251, val loss 2.5436\n",
      "step 1100: train loss 2.5176, val loss 2.5305\n",
      "step 1200: train loss 2.5150, val loss 2.5197\n",
      "step 1300: train loss 2.5030, val loss 2.5246\n",
      "step 1400: train loss 2.5042, val loss 2.5124\n",
      "step 1500: train loss 2.5029, val loss 2.5127\n",
      "step 1600: train loss 2.4964, val loss 2.5111\n",
      "step 1700: train loss 2.4958, val loss 2.5101\n",
      "step 1800: train loss 2.4934, val loss 2.5087\n",
      "step 1900: train loss 2.4910, val loss 2.5093\n",
      "step 2000: train loss 2.4850, val loss 2.5052\n",
      "step 2100: train loss 2.4890, val loss 2.5049\n",
      "step 2200: train loss 2.4886, val loss 2.5054\n",
      "step 2300: train loss 2.4794, val loss 2.5038\n",
      "step 2400: train loss 2.4796, val loss 2.5038\n",
      "step 2500: train loss 2.4791, val loss 2.4984\n",
      "step 2600: train loss 2.4779, val loss 2.4985\n",
      "step 2700: train loss 2.4795, val loss 2.4955\n",
      "step 2800: train loss 2.4728, val loss 2.4958\n",
      "step 2900: train loss 2.4737, val loss 2.4966\n",
      "step 3000: train loss 2.4778, val loss 2.5068\n",
      "step 3100: train loss 2.4735, val loss 2.4956\n",
      "step 3200: train loss 2.4775, val loss 2.4962\n",
      "step 3300: train loss 2.4727, val loss 2.4985\n",
      "step 3400: train loss 2.4803, val loss 2.4985\n",
      "step 3500: train loss 2.4805, val loss 2.4973\n",
      "step 3600: train loss 2.4767, val loss 2.4998\n",
      "step 3700: train loss 2.4753, val loss 2.4851\n",
      "step 3800: train loss 2.4739, val loss 2.4939\n",
      "step 3900: train loss 2.4684, val loss 2.4959\n",
      "step 4000: train loss 2.4650, val loss 2.4967\n",
      "step 4100: train loss 2.4701, val loss 2.4908\n",
      "step 4200: train loss 2.4697, val loss 2.4882\n",
      "step 4300: train loss 2.4702, val loss 2.4962\n",
      "step 4400: train loss 2.4690, val loss 2.4969\n",
      "step 4500: train loss 2.4662, val loss 2.4914\n",
      "step 4600: train loss 2.4670, val loss 2.4870\n",
      "step 4700: train loss 2.4684, val loss 2.4977\n",
      "step 4800: train loss 2.4670, val loss 2.4862\n",
      "step 4900: train loss 2.4702, val loss 2.4894\n",
      "step 4999: train loss 2.4694, val loss 2.4971\n",
      "\n",
      "Tourome wis w\n",
      "Whendilirove! f th cod mu.\n",
      "hin g.\n",
      "HAREORimar gh ncurret ng hibamee yo durakegekiss w the ngr out fus tsoves,-t thoonouspevinds thoiofive, Hanys presche in'th thy y sthathiliee anore ave meim f geno f. mchith 'statug\n",
      "Cous,\n",
      "To Gilin:\n",
      "TES:\n",
      "Th, t thy, giopeatht falosoofeathaithe\n",
      "\n",
      "\n",
      "I thoungleve manothe ind\n",
      "Cofinor, Jur ng, m an aneby ryoremep our Mise whin ow f ciotcou ispllere\n",
      "\n",
      "HUS:\n",
      "gin car Jupotallthy uthy:\n",
      "S:\n",
      "Wif machorbad gerim heth w g.\n",
      "Ayoudsth sue whor idinoly, se, tham her vet\n",
      "EOLAS:\n",
      "ARThystherus riofe:\n",
      "Phathay t pstouis wh s al Hyo h dur cofrnarinome thar, t r mes orouss atheacexis I an f s,\n",
      "An tet wig ctheensemyow anthorat, rt igese lses; joyoulis at we is SAse h tousse\n",
      "Twe, trande,\n",
      "\n",
      "HES:\n",
      "\n",
      "I loy, thachis g velelindl.\n",
      "Bus:\n",
      "\n",
      "T: ow is met: ly lthok t'Thy f metha m.\n",
      "T: t, uls H:\n",
      "LAE oro touplll: thouma I:\n",
      "er pit ooveckioto ook.\n",
      "ARLA:\n",
      "S:\n",
      "Tinckithiowith t heanucuther tlpire rppe hak d tseand hy.\n",
      "AOFFooous werse geamane\n",
      "\n",
      "Bun thu ofr ore h, wan nocouchiveart t oollas th.\n",
      "\n",
      "Fag ath\n",
      "\n",
      "Ay nor beer AUEofomsad h the m ra inol be.\n",
      "PRUEvo aversinthay y, than\n",
      "NARY:\n",
      "\n",
      "Drurtad\n",
      "As mone, allee hinghay booy, ut pier.\n",
      "LI s oro ghetoo cere st, CAUE f u m s, honourricungiound-s angeren bes pry brsprisheresoreshore f t\n",
      "Wer, t-\n",
      "Whon'tt thatoupldulesoft w ale ave th t, thit wallisth nd meaveerefo ee? y r,\n",
      "Gothed ot s\n",
      "INE ltutheatin: hellecl\n",
      "Tndiche f haiou wistiliea t fo lay l otowatole ous\n",
      "WI w FABuchodaove f dior'ds sucout:\n",
      "\n",
      "t t hefoutreag, ct wleammpe ad con haghand ind isithe to pand f Wha held las ang h pry. h athe ma f the ath,\n",
      "IS:\n",
      "K:\n",
      "Thons ssel ply woy,\n",
      "LUS:\n",
      "\n",
      "Who my fe bu th! t ch mmemexan mon qupevers liomar fed anonoYo inthig tin pr amd noff k you wecu nooug hathigillde whige aitoon:\n",
      "G tik mat, mes Whe ldimyomy at waiveanthos woanefell ICa helay th t I h wat:\n",
      "K:\n",
      "\n",
      "ME n. theeik.\n",
      "im ho t ckim\n",
      "He thooupo y. bot co-ave surit.\n",
      "Bu'Bee TINAst Langrin t swe s ffighormesaldslond ay ndeve ndnear. foula ancade ted ELondiserdound beve t tho yonneam su ouiearsithe ghormm l beritold\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e7e7f",
   "metadata": {},
   "source": [
    "## 3. Single self-attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72f3432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022721 M parameters\n",
      "step 0: train loss 4.2104, val loss 4.2112\n",
      "step 100: train loss 3.0323, val loss 3.0592\n",
      "step 200: train loss 2.8618, val loss 2.8798\n",
      "step 300: train loss 2.7799, val loss 2.7993\n",
      "step 400: train loss 2.7349, val loss 2.7378\n",
      "step 500: train loss 2.6847, val loss 2.6834\n",
      "step 600: train loss 2.6242, val loss 2.6307\n",
      "step 700: train loss 2.5742, val loss 2.5813\n",
      "step 800: train loss 2.5220, val loss 2.5225\n",
      "step 900: train loss 2.4832, val loss 2.4967\n",
      "step 1000: train loss 2.4650, val loss 2.4817\n",
      "step 1100: train loss 2.4456, val loss 2.4503\n",
      "step 1200: train loss 2.4321, val loss 2.4392\n",
      "step 1300: train loss 2.4179, val loss 2.4313\n",
      "step 1400: train loss 2.4076, val loss 2.4225\n",
      "step 1500: train loss 2.4012, val loss 2.4204\n",
      "step 1600: train loss 2.4051, val loss 2.4148\n",
      "step 1700: train loss 2.3884, val loss 2.4016\n",
      "step 1800: train loss 2.3876, val loss 2.4063\n",
      "step 1900: train loss 2.3832, val loss 2.3942\n",
      "step 2000: train loss 2.3772, val loss 2.4047\n",
      "step 2100: train loss 2.3805, val loss 2.3997\n",
      "step 2200: train loss 2.3712, val loss 2.3940\n",
      "step 2300: train loss 2.3667, val loss 2.3940\n",
      "step 2400: train loss 2.3624, val loss 2.3829\n",
      "step 2500: train loss 2.3736, val loss 2.3848\n",
      "step 2600: train loss 2.3664, val loss 2.3859\n",
      "step 2700: train loss 2.3653, val loss 2.3779\n",
      "step 2800: train loss 2.3548, val loss 2.3755\n",
      "step 2900: train loss 2.3459, val loss 2.3698\n",
      "step 3000: train loss 2.3588, val loss 2.3747\n",
      "step 3100: train loss 2.3537, val loss 2.3746\n",
      "step 3200: train loss 2.3468, val loss 2.3628\n",
      "step 3300: train loss 2.3461, val loss 2.3720\n",
      "step 3400: train loss 2.3364, val loss 2.3654\n",
      "step 3500: train loss 2.3477, val loss 2.3692\n",
      "step 3600: train loss 2.3468, val loss 2.3747\n",
      "step 3700: train loss 2.3464, val loss 2.3667\n",
      "step 3800: train loss 2.3342, val loss 2.3628\n",
      "step 3900: train loss 2.3288, val loss 2.3662\n",
      "step 4000: train loss 2.3301, val loss 2.3642\n",
      "step 4100: train loss 2.3306, val loss 2.3591\n",
      "step 4200: train loss 2.3288, val loss 2.3614\n",
      "step 4300: train loss 2.3289, val loss 2.3583\n",
      "step 4400: train loss 2.3220, val loss 2.3642\n",
      "step 4500: train loss 2.3223, val loss 2.3616\n",
      "step 4600: train loss 2.3213, val loss 2.3481\n",
      "step 4700: train loss 2.3246, val loss 2.3536\n",
      "step 4800: train loss 2.3204, val loss 2.3459\n",
      "step 4900: train loss 2.3314, val loss 2.3499\n",
      "step 4999: train loss 2.3208, val loss 2.3448\n",
      "\n",
      "Nicut betet, us,\n",
      "YCouth Noucced wietsh the lich yecbor ot asovice soome over som my hacrieghhon dim tonert min bestt wawir-mingoun cowood one geld shur dotith theries you dses to thal mathird this bok tism;\n",
      "I sangindo ber the.\n",
      "\n",
      "HR:one I pe hirth\n",
      "Pi, me peirshat orot hainck, me!\n",
      "\n",
      "AGLoouf tupiut bor tur;\n",
      "Fry;\n",
      "o thor'ld yath\n",
      "But chat,\n",
      "I thait,\n",
      "Teschit ng es tam, fa\n",
      "Pro st\n",
      "I chath mithuld hiverco mino thant her f\n",
      "yof tsh f othaum mus? bo btheu psh peer to lm vimpr.\n",
      "\n",
      "Hodor, lor brene ou nd whad mus myod wor p tene bium ty ser.\n",
      "BOloig--\n",
      "ONF at hort, thee anis Adr: yod dset of thons! O dlo ftet bere nwith's thinllo, makithe the\n",
      "LEASNO:\n",
      "Tow git, ot.\n",
      "Frincon?\n",
      "\n",
      "Carshed ploswou huntl dungu vigoad as chatir hemnand\n",
      "Tour\n",
      "Hadn nocold--di-br om, tllem fo-t ther tame?\n",
      "\n",
      "CHLIUSinlat, on ofraerourth cihnd,\n",
      "Th the gs fo smy ow ther bathtoel my,\n",
      "RO:\n",
      "Be yout agit teness me; Gs atile rier wisall set cang to measis\n",
      "Her'st foroureg, rif briseto,\n",
      "Pes po imn antt alaunkn:\n",
      "The, ay kit sofr tir yef ow cilout ber dedrerre mssu teatr-pohard gs mow a pelenen ss;\n",
      "Thew my bed hin aty itho?\n",
      "\n",
      "Whe hovecughe odo ithistt:\n",
      "ng, bawre,\n",
      "Be\n",
      "Ad fout the want nad her th a hous mot do itrake! Gn athe iscke llk.\n",
      "\n",
      "Bouresare pine thal ne,\n",
      "T:\n",
      "Bleve,\n",
      "Y theathas to\n",
      "ns Pecrobrlif cod cithreve.\n",
      "\n",
      "And ll asen at, ond stheatebret, the forr\n",
      "Whait ge ks, she I GNoug bomears ist sen.\n",
      "LLO:\n",
      "EYLO:\n",
      "f I wond D, Ewe.\n",
      "\n",
      "Toun thes Nof uenawris?Tee be\n",
      "Gso ll, leer sly fromar whe ses come te bul misein alp:at\n",
      "I mbe fre sise myou she ar?\n",
      "\n",
      "Aro nd,\n",
      "Y:\n",
      "To me ld ly Porus?\n",
      "\n",
      "IS:\n",
      "N Ed I Gn's yu.\n",
      "\n",
      "Bulimeprd, whe nowm sow.\n",
      "\n",
      "GRYouth ind, My the our I'tll thing bang aulcal ppe tharveir s cll thiths t the.\n",
      "\n",
      "If yr tewis fat theen bere tol\n",
      "To bre mm, ld manot me th myoverong olther mey pris wite, tofuancklondeabrok.\n",
      "\n",
      "Tyos for, whoung thin pre te werist gand;\n",
      "Mater ther, rorway che su'd-'s whin.\n",
      "\n",
      "Gporod fo s mancor thy ty cer' te lll aven tepl;\n",
      "h wasthen\n",
      "Abou pango won brakinmy, nid tis lwhile.\n",
      "\n",
      "Whe,\n",
      "Paler:\n",
      "Cwato spatwhise\n",
      "GOPRULUThow frod by,\n",
      "A from h\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.sa_head = Head(n_embd)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.sa_head(x) # (B, T, C)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca8e2b",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76144f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022721 M parameters\n",
      "step 0: train loss 4.2319, val loss 4.2284\n",
      "step 100: train loss 2.9697, val loss 2.9860\n",
      "step 200: train loss 2.7325, val loss 2.7445\n",
      "step 300: train loss 2.6457, val loss 2.6642\n",
      "step 400: train loss 2.6017, val loss 2.6090\n",
      "step 500: train loss 2.5735, val loss 2.5722\n",
      "step 600: train loss 2.5301, val loss 2.5347\n",
      "step 700: train loss 2.5001, val loss 2.5106\n",
      "step 800: train loss 2.4662, val loss 2.4663\n",
      "step 900: train loss 2.4334, val loss 2.4444\n",
      "step 1000: train loss 2.4144, val loss 2.4287\n",
      "step 1100: train loss 2.3892, val loss 2.3841\n",
      "step 1200: train loss 2.3672, val loss 2.3707\n",
      "step 1300: train loss 2.3490, val loss 2.3565\n",
      "step 1400: train loss 2.3306, val loss 2.3360\n",
      "step 1500: train loss 2.3205, val loss 2.3332\n",
      "step 1600: train loss 2.3140, val loss 2.3170\n",
      "step 1700: train loss 2.2920, val loss 2.3016\n",
      "step 1800: train loss 2.2812, val loss 2.3002\n",
      "step 1900: train loss 2.2710, val loss 2.2814\n",
      "step 2000: train loss 2.2548, val loss 2.2888\n",
      "step 2100: train loss 2.2495, val loss 2.2755\n",
      "step 2200: train loss 2.2413, val loss 2.2644\n",
      "step 2300: train loss 2.2325, val loss 2.2646\n",
      "step 2400: train loss 2.2269, val loss 2.2536\n",
      "step 2500: train loss 2.2288, val loss 2.2473\n",
      "step 2600: train loss 2.2179, val loss 2.2460\n",
      "step 2700: train loss 2.2096, val loss 2.2312\n",
      "step 2800: train loss 2.1966, val loss 2.2233\n",
      "step 2900: train loss 2.1844, val loss 2.2204\n",
      "step 3000: train loss 2.1888, val loss 2.2184\n",
      "step 3100: train loss 2.1829, val loss 2.2167\n",
      "step 3200: train loss 2.1718, val loss 2.2005\n",
      "step 3300: train loss 2.1684, val loss 2.2050\n",
      "step 3400: train loss 2.1574, val loss 2.1985\n",
      "step 3500: train loss 2.1646, val loss 2.1987\n",
      "step 3600: train loss 2.1607, val loss 2.2061\n",
      "step 3700: train loss 2.1592, val loss 2.1982\n",
      "step 3800: train loss 2.1409, val loss 2.1911\n",
      "step 3900: train loss 2.1337, val loss 2.1935\n",
      "step 4000: train loss 2.1326, val loss 2.1886\n",
      "step 4100: train loss 2.1307, val loss 2.1818\n",
      "step 4200: train loss 2.1209, val loss 2.1830\n",
      "step 4300: train loss 2.1188, val loss 2.1753\n",
      "step 4400: train loss 2.1123, val loss 2.1789\n",
      "step 4500: train loss 2.1089, val loss 2.1720\n",
      "step 4600: train loss 2.1089, val loss 2.1669\n",
      "step 4700: train loss 2.1077, val loss 2.1626\n",
      "step 4800: train loss 2.1035, val loss 2.1589\n",
      "step 4900: train loss 2.1109, val loss 2.1638\n",
      "step 4999: train loss 2.0942, val loss 2.1530\n",
      "\n",
      "Sicke be that some frold\n",
      "I ace done shath plich yeers to masove sorto,\n",
      "Ford yof fay hat yer\n",
      "Ach dis tonlemush lo sto wawir-nth tin comeon the gald shus dotey therries you dres to dram make ownt is bok tishe!\n",
      "Nor apend brite, is wauone then him to is me plirshaty, Want,\n",
      "WARPUORKK: Gow Rom, biof bor tho;\n",
      "Fry mot cond thy My\n",
      "BELDrIN:\n",
      "'I d sio, welch the Ie wiak, an\n",
      "Or ast\n",
      "Icr, the.\n",
      "\n",
      "PuRILANY BOCE to Masnot her fay forsh froth hof a tobo bellukend porints lo vispr.\n",
      "\n",
      "Hodowerld shat hink not do thus my by trop trepl iumpendse,\n",
      "Nod tie-s\n",
      "Once the tou tree ance And dyid dy But you Is teart.\n",
      "\n",
      "That fre neard' mont, Poold\n",
      "ABUSTEO:\n",
      "Yow the mach git, of.\n",
      "Fornconece\n",
      "Frof douldss low stlly u bring a tas chatir hempay,\n",
      "Tho chadn nocold thish to dowllem fort ther.\n",
      "\n",
      "Ifll comgorinlay, on to antou thad have shomus goct I'm: of ther batht-\n",
      "To with rowell itca\n",
      "Falle pst met you tile ried willll sel cantwich tast sartout To sore kirs\n",
      "Ast sean's thee hime a fth waunknot,\n",
      "I's ch liks an tir yof ow cilound'd\n",
      "be dremre yss--Peatrupth arcis moona peeend. Thar tow MIines him any lomu?\n",
      "\n",
      "Why Os ponele! Howith sit.\n",
      "\n",
      "Jus awne, Blo blll tut shwash nad her trou hous both of\n",
      "Icant! Gnobllid shire loper; themare pine thalle.\n",
      "\n",
      "CENTIO:\n",
      "To drer,\n",
      "Rast\n",
      "Thes Puch\n",
      "Thous comedid ce, Frinke lus\n",
      "And at, ond sond beir thave shorr\n",
      "Whave gee farst tell owhe\n",
      "Angays is ife hin lord: forf mot\n",
      "LArD, EDI I'l boble sirt: thiminightee be\n",
      "For a by ner sly from and to me come tel we mis in alpoad\n",
      "I llitf:\n",
      "Beare scand hempe?\n",
      "\n",
      "Ero nip dare in to the Pores?\n",
      "\n",
      "I HARKE:\n",
      "When's yound plieds, row be prm rre.\n",
      "\n",
      "LIOA:\n",
      "I ind, Wherem Paroth lust ing broveaulf loppeat armiishe clorth tos deact.\n",
      "\n",
      "If you ear's awelldeef crear\n",
      "Thane tem. Ta dor Woa thish mooverows olt\n",
      "Say:\n",
      "Howrls with, tof anck. MISabto gitry.\n",
      "A:\n",
      "Thou yfor thy with to dusist; and kin's ct sut roplay cord you-'s wom Sir, I of buts mance\n",
      "Ashat yice ' twill tamen to the;\n",
      "\n",
      "SSY:\n",
      "Ser ou paimy wor draking their tis lord wess?\n",
      "\n",
      "LAPUTING:\n",
      "You and Cwond to swith bus ford bvy to you b\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.sa_heads = MultiHeadAttention(n_head, n_embd // n_head)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.sa_heads(x) # (B, T, C)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c727121",
   "metadata": {},
   "source": [
    "## 5. Adding Feed-Forward Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ab46c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bef7eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026881 M parameters\n",
      "step 0: train loss 4.1892, val loss 4.1898\n",
      "step 100: train loss 2.9667, val loss 2.9977\n",
      "step 200: train loss 2.7191, val loss 2.7361\n",
      "step 300: train loss 2.6334, val loss 2.6477\n",
      "step 400: train loss 2.5871, val loss 2.5963\n",
      "step 500: train loss 2.5438, val loss 2.5422\n",
      "step 600: train loss 2.5135, val loss 2.5134\n",
      "step 700: train loss 2.4809, val loss 2.4861\n",
      "step 800: train loss 2.4581, val loss 2.4707\n",
      "step 900: train loss 2.4290, val loss 2.4333\n",
      "step 1000: train loss 2.4147, val loss 2.4108\n",
      "step 1100: train loss 2.3801, val loss 2.3951\n",
      "step 1200: train loss 2.3699, val loss 2.3635\n",
      "step 1300: train loss 2.3446, val loss 2.3507\n",
      "step 1400: train loss 2.3241, val loss 2.3410\n",
      "step 1500: train loss 2.3159, val loss 2.3270\n",
      "step 1600: train loss 2.2974, val loss 2.3027\n",
      "step 1700: train loss 2.2848, val loss 2.3037\n",
      "step 1800: train loss 2.2687, val loss 2.2905\n",
      "step 1900: train loss 2.2630, val loss 2.2748\n",
      "step 2000: train loss 2.2474, val loss 2.2701\n",
      "step 2100: train loss 2.2369, val loss 2.2578\n",
      "step 2200: train loss 2.2312, val loss 2.2573\n",
      "step 2300: train loss 2.2171, val loss 2.2398\n",
      "step 2400: train loss 2.2043, val loss 2.2290\n",
      "step 2500: train loss 2.2001, val loss 2.2235\n",
      "step 2600: train loss 2.1915, val loss 2.2232\n",
      "step 2700: train loss 2.1787, val loss 2.2108\n",
      "step 2800: train loss 2.1710, val loss 2.2158\n",
      "step 2900: train loss 2.1634, val loss 2.1969\n",
      "step 3000: train loss 2.1577, val loss 2.1948\n",
      "step 3100: train loss 2.1452, val loss 2.1942\n",
      "step 3200: train loss 2.1445, val loss 2.1868\n",
      "step 3300: train loss 2.1423, val loss 2.1779\n",
      "step 3400: train loss 2.1377, val loss 2.1804\n",
      "step 3500: train loss 2.1264, val loss 2.1600\n",
      "step 3600: train loss 2.1197, val loss 2.1633\n",
      "step 3700: train loss 2.1104, val loss 2.1577\n",
      "step 3800: train loss 2.0989, val loss 2.1619\n",
      "step 3900: train loss 2.0978, val loss 2.1477\n",
      "step 4000: train loss 2.0899, val loss 2.1422\n",
      "step 4100: train loss 2.0807, val loss 2.1416\n",
      "step 4200: train loss 2.0784, val loss 2.1413\n",
      "step 4300: train loss 2.0690, val loss 2.1314\n",
      "step 4400: train loss 2.0663, val loss 2.1306\n",
      "step 4500: train loss 2.0656, val loss 2.1285\n",
      "step 4600: train loss 2.0624, val loss 2.1215\n",
      "step 4700: train loss 2.0552, val loss 2.1229\n",
      "step 4800: train loss 2.0475, val loss 2.1082\n",
      "step 4900: train loss 2.0449, val loss 2.1174\n",
      "step 4999: train loss 2.0425, val loss 2.1129\n",
      "\n",
      "The shat'l liveny crocked and Kivesiome,\n",
      "For somf your onem\n",
      "To prien onertusion: sto wawir-ning.\n",
      "OLAL:\n",
      "Bailne grads our doter thersies you dred ivund frover'own are bek tisme!\n",
      "NUSENS:\n",
      "No ARETH:\n",
      "Tuardone wern he that sains firshis of tin,\n",
      "LANIUORTHice avy tourion ber thours,\n",
      "Our show'e; you your crome, your of weshal, nowe!\n",
      "On'l fa\n",
      "On. Wipsir, theriende.\n",
      "\n",
      "KE BOLE ORTH:\n",
      "Sot he cady fits vitetincocius? bor then pers,\n",
      "Theas lieviine. Coodore'ld shat hin he his ins!\n",
      "With'd hip my mel iun treser.\n",
      "BOLEi:\n",
      "Lord; a perreat sok and toornd in deat of thens! O drihf, thoure nestir mos, leat my ithe ish\n",
      "LEdade mach git, of, Frincon?\n",
      "\n",
      "Carpher plass lielstled ut avined her chat hicemcann\n",
      "Tood cadn, woold-peith tointellem!\n",
      "\n",
      "'t ther untrutsy ightinlay, on of and brad cis Buts! AR:\n",
      "Jo de smiood with bathtee butticerowe you ca\n",
      "Fall hess my mys\n",
      "BRiTh ried wipainss therng to marsiks.\n",
      "\n",
      "ROSI:\n",
      "Thore king ise seven a would on a froplaun my cre's chall sof OriWhis now ciloun 'te deeremre masurie, my that cis my hat'edenen swarl wered\n",
      "Whinh forey ithournd tresip telelornyiod's treng,\n",
      "AUNThat lood four his wash nad her the or and oand theriked un and mincere leagl;\n",
      "Aresare pire thalle dom: woe then lerae\n",
      "fort and unce\n",
      "Calif comedinh joking and: thing at,\n",
      "And srear ibrothat, Oeker\n",
      "Wrainnge, fals.\n",
      "\n",
      "I'llow as hears is iseneriel plater\n",
      "forch\n",
      "LEONI:\n",
      "Ell of but gusirffournair, saye be\n",
      "FiRe\n",
      "FIUMMEN RIIVHARD:\n",
      "INGARIIO:\n",
      "The te bur misein alp: Whawelinfer eare mvand he Fell brozen,\n",
      "You sir is the do frient my pradine,\n",
      "Thally.\n",
      "\n",
      "I mostrres be prm and.\n",
      "\n",
      "GLOUCENTES:\n",
      "Buthrem his but us ding bare aulcal ppeating gir incly theems. beld.\n",
      "\n",
      "KING ENewilif withre ficen to mane ard.\n",
      "\n",
      "PLETRBUS:\n",
      "On thom ovarows olthery kingris with,\n",
      "Tof anciles sabropmithy.\n",
      "Afor, whing bary.\n",
      "\n",
      "KANGRARGAE:\n",
      "On Monk ther ther, rop ay cord you-'m inme.\n",
      "\n",
      "Rpiaodlef gre, a hermy thiced' teill tamen to lord?\n",
      "\n",
      "Sokente ourp coo for may lamy, nifen,\n",
      "On!\n",
      "Thresser of your have of Ranwy der cropouthes from by hippy my solkess omint sthore;\n",
      "Thre to co\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.sa_heads = MultiHeadAttention(n_head, n_embd // n_head)\n",
    "\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.sa_heads(x) # (B, T, C)\n",
    "        x          = self.ffwd(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23c285",
   "metadata": {},
   "source": [
    "## 6. Repeat in blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e0b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059777 M parameters\n",
      "step 0: train loss 4.1798, val loss 4.1788\n",
      "step 100: train loss 3.3162, val loss 3.3529\n",
      "step 200: train loss 3.3100, val loss 3.3587\n",
      "step 300: train loss 3.2665, val loss 3.2892\n",
      "step 400: train loss 3.1412, val loss 3.1535\n",
      "step 500: train loss 3.1087, val loss 3.0994\n",
      "step 600: train loss 3.0788, val loss 3.0750\n",
      "step 700: train loss 2.9811, val loss 2.9787\n",
      "step 800: train loss 2.9058, val loss 2.8864\n",
      "step 900: train loss 2.8358, val loss 2.8208\n",
      "step 1000: train loss 2.7218, val loss 2.7191\n",
      "step 1100: train loss 2.6741, val loss 2.6617\n",
      "step 1200: train loss 2.6260, val loss 2.6276\n",
      "step 1300: train loss 2.5968, val loss 2.5960\n",
      "step 1400: train loss 2.5719, val loss 2.5805\n",
      "step 1500: train loss 2.5399, val loss 2.5307\n",
      "step 1600: train loss 2.5331, val loss 2.5300\n",
      "step 1700: train loss 2.5056, val loss 2.5115\n",
      "step 1800: train loss 2.4904, val loss 2.4856\n",
      "step 1900: train loss 2.4726, val loss 2.4759\n",
      "step 2000: train loss 2.4691, val loss 2.4682\n",
      "step 2100: train loss 2.4595, val loss 2.4622\n",
      "step 2200: train loss 2.4573, val loss 2.4489\n",
      "step 2300: train loss 2.4312, val loss 2.4282\n",
      "step 2400: train loss 2.4077, val loss 2.4244\n",
      "step 2500: train loss 2.4231, val loss 2.4292\n",
      "step 2600: train loss 2.4059, val loss 2.4160\n",
      "step 2700: train loss 2.3994, val loss 2.4097\n",
      "step 2800: train loss 2.3932, val loss 2.3930\n",
      "step 2900: train loss 2.3815, val loss 2.3962\n",
      "step 3000: train loss 2.3624, val loss 2.3740\n",
      "step 3100: train loss 2.3633, val loss 2.3755\n",
      "step 3200: train loss 2.3424, val loss 2.3508\n",
      "step 3300: train loss 2.3315, val loss 2.3438\n",
      "step 3400: train loss 2.3119, val loss 2.3359\n",
      "step 3500: train loss 2.3133, val loss 2.3211\n",
      "step 3600: train loss 2.2994, val loss 2.3280\n",
      "step 3700: train loss 2.2918, val loss 2.3114\n",
      "step 3800: train loss 2.2992, val loss 2.3090\n",
      "step 3900: train loss 2.2824, val loss 2.2985\n",
      "step 4000: train loss 2.2688, val loss 2.2837\n",
      "step 4100: train loss 2.2739, val loss 2.2921\n",
      "step 4200: train loss 2.2516, val loss 2.2603\n",
      "step 4300: train loss 2.2456, val loss 2.2664\n",
      "step 4400: train loss 2.2425, val loss 2.2686\n",
      "step 4500: train loss 2.2329, val loss 2.2486\n",
      "step 4600: train loss 2.2157, val loss 2.2422\n",
      "step 4700: train loss 2.2361, val loss 2.2457\n",
      "step 4800: train loss 2.2047, val loss 2.2233\n",
      "step 4900: train loss 2.1954, val loss 2.2255\n",
      "step 4999: train loss 2.1888, val loss 2.2177\n",
      "\n",
      "\n",
      "E:\n",
      "Mefr u jower to therlay lean.\n",
      "\n",
      "NUJULHIA:\n",
      "Hot, hand lost she hent\n",
      "but and tha' sap to san: ou that I felom.\n",
      "\n",
      "BUGMKOPT:\n",
      "HTou withe pesyom ferlc see! lyouter,\n",
      "Se my forp in spumos felal Coithe of boly seets, bed thall to my that thry to pilte;\n",
      "Theree slour mesom I moy ston sitk\n",
      "So far lele senees beas uchic tith,\n",
      "Horn the sair, I gef till hent tat on tha heil\n",
      "'As firl bob?\n",
      "\n",
      "Fe somingt; At liirar dros dusat, d the neear; neg?\n",
      "\n",
      "CORIDDUASABRIN:\n",
      "Bu thif this sunt you to' shore le dive in helfw cicht\n",
      "Rote ane and your wallate jilles, mems me abid\n",
      "t the linivent hy digh mory.\n",
      "\n",
      "KING VEMLRCECTIO:\n",
      "Viik und gayt at thillea.\n",
      "\n",
      "LPIIONGCINGUS:\n",
      "And Du thers, I le shill me thert thes tou hy nou con'd thou lepef:\n",
      "I'l!\n",
      "\n",
      "Yur no Rrave, I wat thee woudd be non bathish tour ta mare tcee woul fun to at oulsm:\n",
      "Mit lakar\n",
      "ant thou raorts I tha with ar Rumt thea ale, tit you mas lexou foor gin: I we;\n",
      "Tondo I hid dlothe of my band\n",
      "And natn tibs Nus in mowce to pight of withe lan, meigsh of\n",
      "gomer thak, tha mairog, the nit will hen youl nik hid pere'd Wlidinged.\n",
      "\n",
      "QULENLIO:\n",
      "I':\n",
      "And citcn.\n",
      "\n",
      "\n",
      "INLAN BBERDSY YCefor.\n",
      "\n",
      "CEN:\n",
      "Nr no now erst pliy, ea by civered\n",
      "Tothe hean the that thy, lllou. CENTRWtim icoous my nollak;\n",
      "And a moyw mn and.\n",
      "\n",
      "KA EMWVEMMRO:\n",
      "Sefe nor Enaw in poveming fa\n",
      "As howons now to\n",
      "B'ims thou he my smouren at tou renafs\n",
      "And sitricoist sens that ey\n",
      "oi wee stpey I thourcol fnor; wut no abs mert will with ar,\n",
      "Mist they ustey thou slayel tis a herio\n",
      "End Ctanw catcloing nond\n",
      "Id rovey cron tis wer; thy grioy tro pend tash' ofon Wsreaol, bimy\n",
      "cage manviund wholme: gawr doul ree pevun fens ust an, is mort ip imom of booll to mild,\n",
      "Who Rof ciny fon dea\n",
      "te in puvoan Masule.\n",
      "\n",
      "As gim, tit thisk mirer ther ath me my geeencir a kleac to mise.\n",
      "\n",
      "LANOREUL:\n",
      "I, en gangan al tower fmins you,\n",
      "Wore hep I thich you sun wo day:\n",
      "Mror Dor thing in hit coule onh at o;---\n",
      "\n",
      "JuoUMARILB:\n",
      "I pit non I, and a ufe mak re? af for wuth frnone witer: whins in hear untuled\n",
      "Lat the ton thanl gent thould, swar;\n",
      "Tel Rathe lomd t\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "        )\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513bb78a",
   "metadata": {},
   "source": [
    "## 7. Introduce residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daa88d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.084737 M parameters\n",
      "step 0: train loss 4.5349, val loss 4.5202\n",
      "step 100: train loss 2.6950, val loss 2.7011\n",
      "step 200: train loss 2.5318, val loss 2.5242\n",
      "step 300: train loss 2.4481, val loss 2.4419\n",
      "step 400: train loss 2.3868, val loss 2.3886\n",
      "step 500: train loss 2.3467, val loss 2.3377\n",
      "step 600: train loss 2.2987, val loss 2.3040\n",
      "step 700: train loss 2.2525, val loss 2.2777\n",
      "step 800: train loss 2.2299, val loss 2.2504\n",
      "step 900: train loss 2.1834, val loss 2.2027\n",
      "step 1000: train loss 2.1592, val loss 2.1902\n",
      "step 1100: train loss 2.1403, val loss 2.1627\n",
      "step 1200: train loss 2.1066, val loss 2.1492\n",
      "step 1300: train loss 2.0833, val loss 2.1240\n",
      "step 1400: train loss 2.0737, val loss 2.1272\n",
      "step 1500: train loss 2.0634, val loss 2.1045\n",
      "step 1600: train loss 2.0309, val loss 2.0970\n",
      "step 1700: train loss 2.0202, val loss 2.0873\n",
      "step 1800: train loss 2.0046, val loss 2.0637\n",
      "step 1900: train loss 1.9954, val loss 2.0574\n",
      "step 2000: train loss 1.9775, val loss 2.0362\n",
      "step 2100: train loss 1.9587, val loss 2.0335\n",
      "step 2200: train loss 1.9615, val loss 2.0338\n",
      "step 2300: train loss 1.9488, val loss 2.0346\n",
      "step 2400: train loss 1.9297, val loss 2.0281\n",
      "step 2500: train loss 1.9287, val loss 2.0289\n",
      "step 2600: train loss 1.9290, val loss 2.0050\n",
      "step 2700: train loss 1.9061, val loss 2.0128\n",
      "step 2800: train loss 1.9029, val loss 2.0081\n",
      "step 2900: train loss 1.8841, val loss 1.9952\n",
      "step 3000: train loss 1.8823, val loss 1.9791\n",
      "step 3100: train loss 1.8633, val loss 1.9856\n",
      "step 3200: train loss 1.8677, val loss 1.9889\n",
      "step 3300: train loss 1.8577, val loss 1.9845\n",
      "step 3400: train loss 1.8551, val loss 1.9792\n",
      "step 3500: train loss 1.8623, val loss 1.9729\n",
      "step 3600: train loss 1.8335, val loss 1.9651\n",
      "step 3700: train loss 1.8325, val loss 1.9647\n",
      "step 3800: train loss 1.8265, val loss 1.9599\n",
      "step 3900: train loss 1.8277, val loss 1.9557\n",
      "step 4000: train loss 1.8307, val loss 1.9484\n",
      "step 4100: train loss 1.8184, val loss 1.9465\n",
      "step 4200: train loss 1.8139, val loss 1.9402\n",
      "step 4300: train loss 1.7972, val loss 1.9473\n",
      "step 4400: train loss 1.7970, val loss 1.9345\n",
      "step 4500: train loss 1.7906, val loss 1.9290\n",
      "step 4600: train loss 1.7893, val loss 1.9383\n",
      "step 4700: train loss 1.7876, val loss 1.9330\n",
      "step 4800: train loss 1.7709, val loss 1.9241\n",
      "step 4900: train loss 1.7914, val loss 1.9296\n",
      "step 4999: train loss 1.7736, val loss 1.9217\n",
      "\n",
      "DUTERBY:\n",
      "Cometady prost, and thy love thrand speak,\n",
      "Lay.\n",
      "\n",
      "ISLANA:\n",
      "Selvaight.\n",
      "Shall sity so far from our parfe.\n",
      "\n",
      "CLARD YORK:\n",
      "Hown Citize! Vevain; thee holt takeng.\n",
      "\n",
      "KLARD:\n",
      "'A:\n",
      "Yet a gart me a mind high prisiarvess day;\n",
      "But the news that'st the that\n",
      "with magh that this succeds:\n",
      "Some tore of divesing thy love.\n",
      "\n",
      "ROMEO:\n",
      "The stat what's the igner, my shou he dut the lie,\n",
      "And herenge more of to theirs from's stilven igattent, of hearth the for beht, crood of that hile stall of thrraw-heseep'd him; here'd mench,\n",
      "Breed wors; Is not raven, and on, evengent they madamished.\n",
      "\n",
      "LADY:\n",
      "I mace crould quart, thou smy didiln;\n",
      "foant thou rast that; what it we you mess,\n",
      "Besicion Mogh, I shall not gones the sups of it?\n",
      "\n",
      "ARGARET:\n",
      "Sefer, whilk nate tible shall mance;\n",
      "Your ravor with: laid me gabood good whicks that all.\n",
      "My grong, which would lond mack,\n",
      "And in live's do'd friever sIIIul,- Vonce.\n",
      "\n",
      "\n",
      "KING RICHARD IIII:\n",
      "Sig, crrow too have here, wither by caver'd. What hear In time,\n",
      "Ray, lord of Edward madcon; for not all my carmoy-sentate.\n",
      "\n",
      "KINCABUTES:\n",
      "I'Gf that that the porr'd.\n",
      "\n",
      "FEDAR:\n",
      "I that my evor'd swilveed, my lard.\n",
      "herse ruirent is mate: Dicors. York'st form\n",
      "ois ceman that thou, thou bray unting by me those a bacome,\n",
      "Mistores our pervits, lay lievist hears\n",
      "pittitusia, thlein;\n",
      "Pif med fover thon time to mest of thereo peace\n",
      "As him on his gove in yecall! & vise, wholm, lover do'd deep:\n",
      "That sham steap, is most in imbed hop by lame\n",
      "This s;\n",
      "Metizes your souchter. Frust In maurs.\n",
      "\n",
      "SAR:\n",
      "O, this with mines theirst she maken their baks.\n",
      "\n",
      "MENCAPH:\n",
      "More molike sofe what that it weref yor you,\n",
      "But a may buljia you suckingday,\n",
      "My lover then tine time use on tames;\n",
      "Shall out pocclarve, my sofe. God, ufter!\n",
      "\n",
      "KEMINGH:\n",
      "Then do scond, my her disswird, and bented\n",
      "Lay them what over? my tebled, swer;\n",
      "Tell they lamer if Fring; fliegness way.\n",
      "Let their.\n",
      "\n",
      "SICHARD II:\n",
      "Sis their, is noly cherese hour claugs's anothattick,\n",
      "Stand wolsure or of deve! Hext chal ouch.\n",
      "Siruc it ving barg; faw lotes; and that peit fo\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x) + x  # We fork off here\n",
    "        x = self.ffwd(x) + x\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "        )\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402fac4a",
   "metadata": {},
   "source": [
    "## 8. Increase inner dimension in feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecdbf272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.159041 M parameters\n",
      "step 0: train loss 4.5632, val loss 4.5501\n",
      "step 100: train loss 2.6242, val loss 2.6368\n",
      "step 200: train loss 2.5005, val loss 2.5124\n",
      "step 300: train loss 2.3973, val loss 2.4213\n",
      "step 400: train loss 2.3344, val loss 2.3598\n",
      "step 500: train loss 2.2989, val loss 2.3104\n",
      "step 600: train loss 2.2559, val loss 2.2818\n",
      "step 700: train loss 2.2162, val loss 2.2451\n",
      "step 800: train loss 2.1836, val loss 2.2037\n",
      "step 900: train loss 2.1347, val loss 2.1702\n",
      "step 1000: train loss 2.1116, val loss 2.1503\n",
      "step 1100: train loss 2.0697, val loss 2.1133\n",
      "step 1200: train loss 2.0395, val loss 2.1084\n",
      "step 1300: train loss 2.0199, val loss 2.0789\n",
      "step 1400: train loss 1.9935, val loss 2.0602\n",
      "step 1500: train loss 1.9638, val loss 2.0417\n",
      "step 1600: train loss 1.9628, val loss 2.0389\n",
      "step 1700: train loss 1.9209, val loss 2.0175\n",
      "step 1800: train loss 1.9152, val loss 2.0140\n",
      "step 1900: train loss 1.8993, val loss 2.0132\n",
      "step 2000: train loss 1.9057, val loss 1.9999\n",
      "step 2100: train loss 1.8642, val loss 1.9869\n",
      "step 2200: train loss 1.8463, val loss 1.9812\n",
      "step 2300: train loss 1.8498, val loss 1.9681\n",
      "step 2400: train loss 1.8399, val loss 1.9754\n",
      "step 2500: train loss 1.8280, val loss 1.9638\n",
      "step 2600: train loss 1.8118, val loss 1.9518\n",
      "step 2700: train loss 1.8082, val loss 1.9418\n",
      "step 2800: train loss 1.7829, val loss 1.9297\n",
      "step 2900: train loss 1.7768, val loss 1.8995\n",
      "step 3000: train loss 1.7889, val loss 1.9208\n",
      "step 3100: train loss 1.7704, val loss 1.9088\n",
      "step 3200: train loss 1.7828, val loss 1.9213\n",
      "step 3300: train loss 1.7548, val loss 1.9051\n",
      "step 3400: train loss 1.7592, val loss 1.8975\n",
      "step 3500: train loss 1.7460, val loss 1.9082\n",
      "step 3600: train loss 1.7517, val loss 1.8997\n",
      "step 3700: train loss 1.7295, val loss 1.9083\n",
      "step 3800: train loss 1.7253, val loss 1.8872\n",
      "step 3900: train loss 1.7135, val loss 1.8750\n",
      "step 4000: train loss 1.7151, val loss 1.8603\n",
      "step 4100: train loss 1.7071, val loss 1.8731\n",
      "step 4200: train loss 1.7094, val loss 1.8821\n",
      "step 4300: train loss 1.7011, val loss 1.8820\n",
      "step 4400: train loss 1.7023, val loss 1.8720\n",
      "step 4500: train loss 1.6914, val loss 1.8677\n",
      "step 4600: train loss 1.6888, val loss 1.8492\n",
      "step 4700: train loss 1.6850, val loss 1.8453\n",
      "step 4800: train loss 1.6886, val loss 1.8537\n",
      "step 4900: train loss 1.6872, val loss 1.8556\n",
      "step 4999: train loss 1.6876, val loss 1.8502\n",
      "\n",
      "That 'tway yourself, worth irnhing well: and I! I in in the prial whe clord, that share catchieoo, wist deadly, laway's alactors of God'st\n",
      "That is thiess of you wook was it.\n",
      "\n",
      "CORLUNIUS:\n",
      "Rome letter the acragr.\n",
      "\n",
      "DUKE VINGEO:\n",
      "The dock!\n",
      "Shold make are bart, for this abre to his king yet's shall:\n",
      "Arlok she forth he were budder\n",
      "Romeet, whame thus? I'll queet to the share thhen forst I but honours?\n",
      "\n",
      "RICHARD:\n",
      "In you that chold gold frhatendled bed\n",
      "Dasiderether cardy good but charce, by Lewast his allow; unkentling give Unething pent\n",
      "Balled beor, hims: it I cond shore buthen dok death of colds, Edward! my Give him entectards. This those art the king the thou hart\n",
      "His obstaxreds the Pentoss oncey than:\n",
      "Hrong, you hand! shall, lack on mine harfed by old ellow inFully.\n",
      "\n",
      "RICHARD:\n",
      "Then I was reast that\n",
      "Let's impts rison.\n",
      "\n",
      "CORIOLANUS:\n",
      "How not not dread Mentley her.\n",
      "\n",
      "Farewher Caced:\n",
      "Come set shir kpile, that far my bound:\n",
      "Manot that of thrush.\n",
      "But Latuale: that is there: I baspetter I pardon.\n",
      "Come as your the evere's it banitiful;\n",
      "Shappir eneven's more the cursely solic:\n",
      "Lose a betchoaly, ill work not. Let I am\n",
      "Is you warth a hambook!\n",
      "All gracest your more andlersen of boith,\n",
      "And lop this oar us, but the--\n",
      "Acking an shown the volry blown;\n",
      "My fath gold a the hast this upof thise face,\n",
      "Or doth to ane years. Whatwer hitten king.\n",
      "\n",
      "SheBRIUS:\n",
      "Well, shall, hathis eals.\n",
      "\n",
      "FLORIE:\n",
      "March, juitt least thence in I,\n",
      "And thou hart pust the carse thy from minstrok he banishat\n",
      "Tome hereis, his begined here it whim hosters\n",
      "He beee if heldreds by, I becanE I dease: mo haster but as bold,\n",
      "And be to great entlers ove as they in felling you are lip-had.\n",
      "\n",
      "ISLAND:\n",
      "Yeal I do bes it not is\n",
      "For hast night it lo are his hught for mentle,\n",
      "Who pais good on more that to make is friends king; that hath bet that not you pater.\n",
      "\n",
      "AUTOLYCUS:\n",
      "What It strolinge, wifed your holdrance,\n",
      "That taste as his: I hable that I duked?\n",
      "I epraust happy blentle, that but unter?\n",
      "Pay giver:\n",
      "Hizer\n",
      "for it mertate, good I prapre: wo yea\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x) + x  # We fork off here\n",
    "        x = self.ffwd(x) + x\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "        )\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7819873",
   "metadata": {},
   "source": [
    "## 9. LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f430cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.159937 M parameters\n",
      "step 0: train loss 4.3390, val loss 4.3285\n",
      "step 100: train loss 2.6798, val loss 2.6896\n",
      "step 200: train loss 2.5082, val loss 2.5202\n",
      "step 300: train loss 2.4298, val loss 2.4429\n",
      "step 400: train loss 2.3633, val loss 2.3748\n",
      "step 500: train loss 2.3119, val loss 2.3152\n",
      "step 600: train loss 2.2688, val loss 2.2849\n",
      "step 700: train loss 2.2328, val loss 2.2504\n",
      "step 800: train loss 2.2134, val loss 2.2208\n",
      "step 900: train loss 2.1645, val loss 2.1841\n",
      "step 1000: train loss 2.1375, val loss 2.1605\n",
      "step 1100: train loss 2.0994, val loss 2.1297\n",
      "step 1200: train loss 2.0717, val loss 2.1137\n",
      "step 1300: train loss 2.0538, val loss 2.0889\n",
      "step 1400: train loss 2.0278, val loss 2.0769\n",
      "step 1500: train loss 2.0078, val loss 2.0561\n",
      "step 1600: train loss 1.9874, val loss 2.0467\n",
      "step 1700: train loss 1.9643, val loss 2.0347\n",
      "step 1800: train loss 1.9544, val loss 2.0295\n",
      "step 1900: train loss 1.9411, val loss 2.0278\n",
      "step 2000: train loss 1.9321, val loss 2.0071\n",
      "step 2100: train loss 1.9056, val loss 2.0070\n",
      "step 2200: train loss 1.8830, val loss 1.9963\n",
      "step 2300: train loss 1.8757, val loss 1.9803\n",
      "step 2400: train loss 1.8596, val loss 1.9811\n",
      "step 2500: train loss 1.8523, val loss 1.9641\n",
      "step 2600: train loss 1.8295, val loss 1.9555\n",
      "step 2700: train loss 1.8310, val loss 1.9556\n",
      "step 2800: train loss 1.8082, val loss 1.9400\n",
      "step 2900: train loss 1.8083, val loss 1.9140\n",
      "step 3000: train loss 1.8081, val loss 1.9277\n",
      "step 3100: train loss 1.7910, val loss 1.9244\n",
      "step 3200: train loss 1.7876, val loss 1.9185\n",
      "step 3300: train loss 1.7764, val loss 1.9190\n",
      "step 3400: train loss 1.7754, val loss 1.9024\n",
      "step 3500: train loss 1.7652, val loss 1.9132\n",
      "step 3600: train loss 1.7569, val loss 1.8945\n",
      "step 3700: train loss 1.7388, val loss 1.9035\n",
      "step 3800: train loss 1.7366, val loss 1.8926\n",
      "step 3900: train loss 1.7258, val loss 1.8744\n",
      "step 4000: train loss 1.7230, val loss 1.8669\n",
      "step 4100: train loss 1.7191, val loss 1.8754\n",
      "step 4200: train loss 1.7184, val loss 1.8787\n",
      "step 4300: train loss 1.7118, val loss 1.8768\n",
      "step 4400: train loss 1.7067, val loss 1.8678\n",
      "step 4500: train loss 1.6940, val loss 1.8697\n",
      "step 4600: train loss 1.6950, val loss 1.8532\n",
      "step 4700: train loss 1.6943, val loss 1.8495\n",
      "step 4800: train loss 1.6899, val loss 1.8498\n",
      "step 4900: train loss 1.6893, val loss 1.8461\n",
      "step 4999: train loss 1.6853, val loss 1.8480\n",
      "\n",
      "That 't\n",
      "Boly, look lest that now, so.\n",
      "If CAPELLO:\n",
      "Welill this waring\n",
      "This fintle has rance: your fellow dead he ladly shall.\n",
      "\n",
      "ROMEO:\n",
      "I 'Fell not will,ered his was this prit.\n",
      "\n",
      "CLAUTES:\n",
      "Now.\n",
      "\n",
      "MERCUTIO:\n",
      "Yea ladruse.\n",
      "\n",
      "PAULINA:\n",
      "There.\n",
      "\n",
      "Mystlo, grate:\n",
      "Hast wast from not but to his known fortus burning,\n",
      "Whence'd be he I'll budder\n",
      "Romeet, what must thou wings,\n",
      "Merchils thrist husbonds thou his cousin.\n",
      "\n",
      "Clown:\n",
      "A wort lay your deards the surn let bed\n",
      "Dastend the bothout oo hears hath table foust his allow; sweet Conour;\n",
      "O mark by face heavely but all\n",
      "As grounds on this: but thost forth, I acch, foulu.\n",
      "\n",
      "Clown:\n",
      "And if that say, you it these what you\n",
      "As a hear his brack your thirster I\n",
      "But themion.\n",
      "\n",
      "MORALEANUS:\n",
      "O, 'Twell I's than you\n",
      "When so What the newly ell are you, let it\n",
      "What not fourth; but But the brother, so, I'll this I will it.\n",
      "\n",
      "CAPELO:\n",
      "O sure at this rack?\n",
      "He hear the cantle serve in this tongul,\n",
      "In my belloodly of thou offenrued.\n",
      "\n",
      "Four the efrought; go your losesperdly to burny.\n",
      "Comestay, look-carrow he it banish.\n",
      "\n",
      "AUTENES:\n",
      "Weleven's more the curdow'd:\n",
      "But those a detchoaly, ill\n",
      "worse do'd was a chack, buritter, you bour this farest tward nate?\n",
      "Yur his of belieks men, proed to bell, but the quecant?\n",
      "\n",
      "GAUTAUSET:\n",
      "Sere's behing and hell ge, a that neg thirsure you: evenge, lord this dangere,\n",
      "And beat it, that knish! I'll we give brother late, is eals.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Gentle that nock in I,\n",
      "And thou hardst she cuccess of your oblins conto Macking the myself is this begine:\n",
      "I kill not me's it elROD:\n",
      "Jet come Enress your becince:\n",
      "What this haste the take old,\n",
      "And be to goodone hands ove: ser that she was his die.\n",
      "\n",
      "LUCIO:\n",
      "If Ghow threat.\n",
      "\n",
      "MERTES:\n",
      "Nor in so her Thurn of diet\n",
      "TRach his hugh'd not mark! the wife, good, look,\n",
      "Tis hat that is yours on knabrate beholdy It this not you;\n",
      "Ondrus dothing for than the lord is doing'd not hear daughter,\n",
      "By still the carchible that I guall?\n",
      "I eprace.\n",
      "\n",
      "EGELO:\n",
      "In my myself Cliffles?\n",
      "\n",
      "Shignirr:\n",
      "His a\n",
      "facier me this a lawn; you are, of yo\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1  = nn.LayerNorm(n_embd)\n",
    "        self.ln2  = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))   # We fork off here\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=n_head),\n",
    "            Block(n_embd, n_head=n_head),\n",
    "            Block(n_embd, n_head=n_head),\n",
    "            nn.LayerNorm(n_embd)\n",
    "        )\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9728927c",
   "metadata": {},
   "source": [
    "## 10. Scale it up by stacking blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab316889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.3453, val loss 4.3315\n",
      "step 100: train loss 2.6522, val loss 2.6653\n",
      "step 200: train loss 2.5082, val loss 2.5003\n",
      "step 300: train loss 2.4304, val loss 2.4419\n",
      "step 400: train loss 2.3503, val loss 2.3572\n",
      "step 500: train loss 2.3087, val loss 2.3209\n",
      "step 600: train loss 2.2584, val loss 2.2635\n",
      "step 700: train loss 2.2183, val loss 2.2328\n",
      "step 800: train loss 2.1812, val loss 2.2054\n",
      "step 900: train loss 2.1381, val loss 2.1670\n",
      "step 1000: train loss 2.1166, val loss 2.1414\n",
      "step 1100: train loss 2.0893, val loss 2.1342\n",
      "step 1200: train loss 2.0492, val loss 2.0888\n",
      "step 1300: train loss 2.0347, val loss 2.0723\n",
      "step 1400: train loss 2.0036, val loss 2.0475\n",
      "step 1500: train loss 1.9808, val loss 2.0428\n",
      "step 1600: train loss 1.9561, val loss 2.0392\n",
      "step 1700: train loss 1.9495, val loss 2.0199\n",
      "step 1800: train loss 1.9105, val loss 2.0077\n",
      "step 1900: train loss 1.9019, val loss 1.9820\n",
      "step 2000: train loss 1.8873, val loss 1.9998\n",
      "step 2100: train loss 1.8644, val loss 1.9757\n",
      "step 2200: train loss 1.8499, val loss 1.9588\n",
      "step 2300: train loss 1.8451, val loss 1.9555\n",
      "step 2400: train loss 1.8370, val loss 1.9414\n",
      "step 2500: train loss 1.8057, val loss 1.9343\n",
      "step 2600: train loss 1.8055, val loss 1.9218\n",
      "step 2700: train loss 1.8044, val loss 1.9347\n",
      "step 2800: train loss 1.7947, val loss 1.9204\n",
      "step 2900: train loss 1.7845, val loss 1.9157\n",
      "step 3000: train loss 1.7771, val loss 1.9095\n",
      "step 3100: train loss 1.7552, val loss 1.9109\n",
      "step 3200: train loss 1.7437, val loss 1.9034\n",
      "step 3300: train loss 1.7456, val loss 1.8976\n",
      "step 3400: train loss 1.7374, val loss 1.8855\n",
      "step 3500: train loss 1.7268, val loss 1.8876\n",
      "step 3600: train loss 1.7214, val loss 1.8854\n",
      "step 3700: train loss 1.7187, val loss 1.8758\n",
      "step 3800: train loss 1.7059, val loss 1.8857\n",
      "step 3900: train loss 1.7039, val loss 1.8646\n",
      "step 4000: train loss 1.6959, val loss 1.8581\n",
      "step 4100: train loss 1.6953, val loss 1.8731\n",
      "step 4200: train loss 1.6876, val loss 1.8554\n",
      "step 4300: train loss 1.6888, val loss 1.8468\n",
      "step 4400: train loss 1.6940, val loss 1.8597\n",
      "step 4500: train loss 1.6833, val loss 1.8557\n",
      "step 4600: train loss 1.6737, val loss 1.8294\n",
      "step 4700: train loss 1.6671, val loss 1.8378\n",
      "step 4800: train loss 1.6491, val loss 1.8299\n",
      "step 4900: train loss 1.6534, val loss 1.8322\n",
      "step 4999: train loss 1.6496, val loss 1.8198\n",
      "\n",
      "Flighter that I have humble bidite me;\n",
      "Say dom he enries begghart I am, it\n",
      "What satell no fore to galls; then of this Murder;\n",
      "And thou earts your may trust that,\n",
      "Or Were for mark of confrencestand Nay.\n",
      "\n",
      "PARTILLA:\n",
      "Behoure wee fear?\n",
      "\n",
      "Shat Marry,\n",
      "For many porte,\n",
      "Ry lame out, as where mayself thus faceess do spost I vou goom own give\n",
      "So to go the rivise trough, which wranger his have dree and\n",
      "Hy that now, stoldo, bethen I would and oath, but all\n",
      "My sworge to and awaughts,\n",
      "As to sove is the wilts gried me,\n",
      "And fliems, my confulle, nob to-birron, Sor led to be mark your tonger shall too genere men\n",
      "Come much an musine.\n",
      "\n",
      "KING RICHARD II:\n",
      "At I enfer staught at the hast long.\n",
      "\n",
      "ISABELLO:\n",
      "Tidder spy you,\n",
      "Gold our in to shall he cannote;\n",
      "How at your our stluge to doth, my troyal added\n",
      "For you, chis let withman, but therefath;\n",
      "He not the with tormsine son!\n",
      "Thee, ten, conded. If the welp age. What? whose crive in your your:\n",
      "I shaull oarth wen Rome, for bee gify;\n",
      "Not do these neir freetishmy word his I dry affleasurener thou would may\n",
      "Withere here, if as not, but no, thy lasbelike\n",
      "But these it were is the strong, and forth stest both sA jreet:\n",
      "Whis must dost the keepled to thy will doth in andier;\n",
      "None, I am malke now, that gentle with bethere may betish end alweet's boy exides'd.\n",
      "\n",
      "ISABELLA:\n",
      "Certuse the slands you there? I should of or contrescannotsegn.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "By chardly hus. Warwick\n",
      "Put Volless shee, all, and greats\n",
      "Aumate, as be dieds fill it a thee?\n",
      "as out our binged. Where and see Yet,\n",
      "All trarmide toble anows mon the withre's with them usweet.\n",
      "\n",
      "CORIOLANUS:\n",
      "Lo, where 'twon elmer, 'morth young age\n",
      "With firszartunest, if the here?\n",
      "\n",
      "BOLONGELLO:\n",
      "As the madaid sap with they suish, good so trueise the die.\n",
      "\n",
      "COMINIUS:\n",
      "Or BoBRiK:\n",
      "Os face, ese servy in eass in I that men do the wards age so sent-form'd:\n",
      "Uwn jurphate, woess no thums of the shin,\n",
      "Will nney: far it mont?\n",
      "\n",
      "Messervanchating and we sterving\n",
      "Butisceized are to men the touch'd,\n",
      "King Lentletwer may beninefire tark\n",
      "Th\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1  = nn.LayerNorm(n_embd)\n",
    "        self.ln2  = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))   # We fork off here\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        x          = self.ln_f(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67154294",
   "metadata": {},
   "source": [
    "## 11. Enable dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a4895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.3453, val loss 4.3315\n",
      "step 100: train loss 2.6522, val loss 2.6653\n",
      "step 200: train loss 2.5082, val loss 2.5003\n",
      "step 300: train loss 2.4304, val loss 2.4419\n",
      "step 400: train loss 2.3503, val loss 2.3572\n",
      "step 500: train loss 2.3087, val loss 2.3209\n",
      "step 600: train loss 2.2584, val loss 2.2635\n",
      "step 700: train loss 2.2183, val loss 2.2328\n",
      "step 800: train loss 2.1812, val loss 2.2054\n",
      "step 900: train loss 2.1381, val loss 2.1670\n",
      "step 1000: train loss 2.1166, val loss 2.1414\n",
      "step 1100: train loss 2.0893, val loss 2.1342\n",
      "step 1200: train loss 2.0492, val loss 2.0888\n",
      "step 1300: train loss 2.0347, val loss 2.0723\n",
      "step 1400: train loss 2.0036, val loss 2.0475\n",
      "step 1500: train loss 1.9808, val loss 2.0428\n",
      "step 1600: train loss 1.9561, val loss 2.0392\n",
      "step 1700: train loss 1.9495, val loss 2.0199\n",
      "step 1800: train loss 1.9105, val loss 2.0077\n",
      "step 1900: train loss 1.9019, val loss 1.9820\n",
      "step 2000: train loss 1.8873, val loss 1.9998\n",
      "step 2100: train loss 1.8644, val loss 1.9757\n",
      "step 2200: train loss 1.8499, val loss 1.9588\n",
      "step 2300: train loss 1.8451, val loss 1.9555\n",
      "step 2400: train loss 1.8370, val loss 1.9414\n",
      "step 2500: train loss 1.8057, val loss 1.9343\n",
      "step 2600: train loss 1.8055, val loss 1.9218\n",
      "step 2700: train loss 1.8044, val loss 1.9347\n",
      "step 2800: train loss 1.7947, val loss 1.9204\n",
      "step 2900: train loss 1.7845, val loss 1.9157\n",
      "step 3000: train loss 1.7771, val loss 1.9095\n",
      "step 3100: train loss 1.7552, val loss 1.9109\n",
      "step 3200: train loss 1.7437, val loss 1.9034\n",
      "step 3300: train loss 1.7456, val loss 1.8976\n",
      "step 3400: train loss 1.7374, val loss 1.8855\n",
      "step 3500: train loss 1.7268, val loss 1.8876\n",
      "step 3600: train loss 1.7214, val loss 1.8854\n",
      "step 3700: train loss 1.7187, val loss 1.8758\n",
      "step 3800: train loss 1.7059, val loss 1.8857\n",
      "step 3900: train loss 1.7039, val loss 1.8646\n",
      "step 4000: train loss 1.6959, val loss 1.8581\n",
      "step 4100: train loss 1.6953, val loss 1.8731\n",
      "step 4200: train loss 1.6876, val loss 1.8554\n",
      "step 4300: train loss 1.6888, val loss 1.8468\n",
      "step 4400: train loss 1.6940, val loss 1.8597\n",
      "step 4500: train loss 1.6833, val loss 1.8557\n",
      "step 4600: train loss 1.6737, val loss 1.8294\n",
      "step 4700: train loss 1.6671, val loss 1.8378\n",
      "step 4800: train loss 1.6491, val loss 1.8299\n",
      "step 4900: train loss 1.6534, val loss 1.8322\n",
      "step 4999: train loss 1.6496, val loss 1.8198\n",
      "\n",
      "Flighter that I have humble bidite me;\n",
      "Say dom he enries begghart I am, it\n",
      "What satell no fore to galls; then of this Murder;\n",
      "And thou earts your may trust that,\n",
      "Or Were for mark of confrencestand Nay.\n",
      "\n",
      "PARTILLA:\n",
      "Behoure wee fear?\n",
      "\n",
      "Shat Marry,\n",
      "For many porte,\n",
      "Ry lame out, as where mayself thus faceess do spost I vou goom own give\n",
      "So to go the rivise trough, which wranger his have dree and\n",
      "Hy that now, stoldo, bethen I would and oath, but all\n",
      "My sworge to and awaughts,\n",
      "As to sove is the wilts gried me,\n",
      "And fliems, my confulle, nob to-birron, Sor led to be mark your tonger shall too genere men\n",
      "Come much an musine.\n",
      "\n",
      "KING RICHARD II:\n",
      "At I enfer staught at the hast long.\n",
      "\n",
      "ISABELLO:\n",
      "Tidder spy you,\n",
      "Gold our in to shall he cannote;\n",
      "How at your our stluge to doth, my troyal added\n",
      "For you, chis let withman, but therefath;\n",
      "He not the with tormsine son!\n",
      "Thee, ten, conded. If the welp age. What? whose crive in your your:\n",
      "I shaull oarth wen Rome, for bee gify;\n",
      "Not do these neir freetishmy word his I dry affleasurener thou would may\n",
      "Withere here, if as not, but no, thy lasbelike\n",
      "But these it were is the strong, and forth stest both sA jreet:\n",
      "Whis must dost the keepled to thy will doth in andier;\n",
      "None, I am malke now, that gentle with bethere may betish end alweet's boy exides'd.\n",
      "\n",
      "ISABELLA:\n",
      "Certuse the slands you there? I should of or contrescannotsegn.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "By chardly hus. Warwick\n",
      "Put Volless shee, all, and greats\n",
      "Aumate, as be dieds fill it a thee?\n",
      "as out our binged. Where and see Yet,\n",
      "All trarmide toble anows mon the withre's with them usweet.\n",
      "\n",
      "CORIOLANUS:\n",
      "Lo, where 'twon elmer, 'morth young age\n",
      "With firszartunest, if the here?\n",
      "\n",
      "BOLONGELLO:\n",
      "As the madaid sap with they suish, good so trueise the die.\n",
      "\n",
      "COMINIUS:\n",
      "Or BoBRiK:\n",
      "Os face, ese servy in eass in I that men do the wards age so sent-form'd:\n",
      "Uwn jurphate, woess no thums of the shin,\n",
      "Will nney: far it mont?\n",
      "\n",
      "Messervanchating and we sterving\n",
      "Butisceized are to men the touch'd,\n",
      "King Lentletwer may beninefire tark\n",
      "Th\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple layers of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1  = nn.LayerNorm(n_embd)\n",
    "        self.ln2  = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))   # We fork off here\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # go from embedding layer to vocabulary\n",
    "        self.ln_head  = nn.Linear(n_embd, vocab_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd   = self.position_embedding_table(torch.arange(T, device=device)) # (T, c)\n",
    "        x          = token_embd + pos_embd # (B, T, C)\n",
    "        x          = self.blocks(x)\n",
    "        x          = self.ln_f(x)\n",
    "        logits     = self.ln_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # crop context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca17098",
   "metadata": {},
   "source": [
    "## 12. Crank up hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6c8c3",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59777ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f2ae84",
   "metadata": {},
   "source": [
    "10 M parameters - my laptop is not qualified to run this so I outsourced the computation to Colab. \n",
    "\n",
    "The final parameters after 1.5h of training on Colab's T4 GPU can be found in `bigram_language_model.pt`. \n",
    "The val loss decreased to 1.48 instead of the promised 1.4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a66132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# save model weights\n",
    "torch.save(model.state_dict(), \"bigram_language_model.pt\")\n",
    "\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c297bd21",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "10.788929 M parameters\n",
    "step 0: train loss 4.2849, val loss 4.2823\n",
    "step 100: train loss 2.4737, val loss 2.4894\n",
    "step 200: train loss 2.4166, val loss 2.4449\n",
    "step 300: train loss 2.3243, val loss 2.3535\n",
    "step 400: train loss 2.1415, val loss 2.1922\n",
    "step 500: train loss 2.0014, val loss 2.0889\n",
    "step 600: train loss 1.8856, val loss 2.0050\n",
    "step 700: train loss 1.7885, val loss 1.9325\n",
    "step 800: train loss 1.7125, val loss 1.8648\n",
    "step 900: train loss 1.6496, val loss 1.8185\n",
    "step 1000: train loss 1.6009, val loss 1.7823\n",
    "step 1100: train loss 1.5601, val loss 1.7425\n",
    "step 1200: train loss 1.5205, val loss 1.7134\n",
    "step 1300: train loss 1.4902, val loss 1.6866\n",
    "step 1400: train loss 1.4617, val loss 1.6604\n",
    "step 1500: train loss 1.4355, val loss 1.6420\n",
    "step 1600: train loss 1.4193, val loss 1.6291\n",
    "step 1700: train loss 1.3949, val loss 1.6064\n",
    "step 1800: train loss 1.3762, val loss 1.5990\n",
    "step 1900: train loss 1.3586, val loss 1.5862\n",
    "step 2000: train loss 1.3464, val loss 1.5750\n",
    "step 2100: train loss 1.3262, val loss 1.5569\n",
    "step 2200: train loss 1.3192, val loss 1.5573\n",
    "step 2300: train loss 1.3023, val loss 1.5403\n",
    "step 2400: train loss 1.2898, val loss 1.5282\n",
    "step 2500: train loss 1.2778, val loss 1.5290\n",
    "step 2600: train loss 1.2677, val loss 1.5238\n",
    "step 2700: train loss 1.2563, val loss 1.5227\n",
    "step 2800: train loss 1.2495, val loss 1.5156\n",
    "step 2900: train loss 1.2402, val loss 1.5067\n",
    "step 3000: train loss 1.2289, val loss 1.5020\n",
    "step 3100: train loss 1.2147, val loss 1.4980\n",
    "step 3200: train loss 1.2098, val loss 1.5005\n",
    "step 3300: train loss 1.2022, val loss 1.4973\n",
    "step 3400: train loss 1.1898, val loss 1.4899\n",
    "step 3500: train loss 1.1824, val loss 1.4885\n",
    "step 3600: train loss 1.1739, val loss 1.4897\n",
    "step 3700: train loss 1.1687, val loss 1.4864\n",
    "step 3800: train loss 1.1622, val loss 1.4862\n",
    "step 3900: train loss 1.1550, val loss 1.4845\n",
    "step 4000: train loss 1.1477, val loss 1.4731\n",
    "step 4100: train loss 1.1376, val loss 1.4873\n",
    "step 4200: train loss 1.1316, val loss 1.4856\n",
    "step 4300: train loss 1.1223, val loss 1.4773\n",
    "step 4400: train loss 1.1140, val loss 1.4747\n",
    "step 4500: train loss 1.1114, val loss 1.4809\n",
    "step 4600: train loss 1.1018, val loss 1.4737\n",
    "step 4700: train loss 1.0996, val loss 1.4800\n",
    "step 4800: train loss 1.0885, val loss 1.4769\n",
    "step 4900: train loss 1.0820, val loss 1.4728\n",
    "step 4999: train loss 1.0723, val loss 1.4800\n",
    "\n",
    "But with prison: I will steed for those\n",
    "meet wounds against the fury hours of Runer.\n",
    "\n",
    "Third Citizen:\n",
    "\n",
    "COMINIUS:\n",
    "Alrue, Marcius, &C this about of cupbol\n",
    "That beg the Capulet's harm, and with lessorr'd:\n",
    "All comparts an enrichly serviced that lie.\n",
    "\n",
    "First Servingman:\n",
    "How! what now this?\n",
    "\n",
    "MARIANA:\n",
    "Better, to it.\n",
    "\n",
    "First GREEN:\n",
    "Ay my lord, our fingers.\n",
    "\n",
    "ISABELLA:\n",
    "As I would fellow you go to sup,\n",
    "That wear Claudio were here: lethen\n",
    "About unstranch'd your holdiness richery,\n",
    "Infraitify the profins\n",
    "Of functions.\n",
    "\n",
    "AUTOLYCUS:\n",
    "Ere you hear go to Clarence,\n",
    "And you yet here hear me but his curfeit is fork.\n",
    "\n",
    "ESCALUS:\n",
    "He gift! be a unhappy thrifty bid this first,\n",
    "Save become by him my walls.\n",
    "\n",
    "LEONTES:\n",
    "Ever.\n",
    "\n",
    "LEONTES:\n",
    "Poor wife.\n",
    "\n",
    "Second Perdart:\n",
    "You would go what you might meas for your life at outwn.\n",
    "\n",
    "POLIXENES:\n",
    "You shall hath been force far?\n",
    "\n",
    "Chown:\n",
    "Where, that I had but thought which thou they say\n",
    "You pleaseme in stay on the vauch courage of Bohemond?\n",
    "\n",
    "POLIXENES:\n",
    "Never did!\n",
    "\n",
    "HERBELLIS:\n",
    "There will not be no more!\n",
    "Better her than I turn the brother's heart\n",
    "Is altender that he loves me will make,\n",
    "I fashiot sough for soldier which shore these wearisoes:\n",
    "My true are the limps of my loyal part;\n",
    "But that but brings was this hasteless, babes wherewered,\n",
    "And here all the temples there he carries\n",
    "At which Jean makes will leads.\n",
    "\n",
    "First Kue, Are you are my common unto\n",
    "To longer Jume than good my liege, but dhere\n",
    "Is maidenhead to the deep: what I have like.\n",
    "\n",
    "WARWICK:\n",
    "Soldom for all him to visatory,\n",
    "And give us see to, fortune slew to chaive me letter.\n",
    "\n",
    "CLIFFORD:\n",
    "Why, I so fear, but see, they say, let's tale the prince.\n",
    "\n",
    "RIVERS:\n",
    "Come forthwith Richard: these wall, go to harm.\n",
    "Thy father shall hearing, I fear: them are\n",
    "My noble house old Rome.\n",
    "\n",
    "CLAUDIO:\n",
    "I heardear, so die thee.\n",
    "\n",
    "ISABELLA:\n",
    "Being that I have wonder'd thyself and extress\n",
    "Because thee Dauncause of your Clarence,\n",
    "\n",
    "CLAUDIO:\n",
    "Who dares thou hast offers my own acching?\n",
    "\n",
    "Lord Germio, Prince Margaret, applaint,\n",
    "Come that we beaten, with us \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c32a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If you have our gift behind the foul counsel:\n",
      "Ere their more destruction shall possessess\n",
      "A prunes more with and millsome strives!\n",
      "\n",
      "HASTINGS:\n",
      "Come, that fetch his sword will descrupt herein.\n",
      "\n",
      "SLY:\n",
      "I have heard them alas! These royal grounds\n",
      "Shall humbly hie these plainly gives him grant\n",
      "Be his disdained but ease. Come, let them go.\n",
      "\n",
      "Third Citizen:\n",
      "Prodixenes,\n",
      "No, not impediment needs favour of his curse.\n",
      "\n",
      "MENENIUS:\n",
      "Hark you, sir?\n",
      "\n",
      "CORIOLANUS:\n",
      "I pray you, go you not.\n",
      "\n",
      "BRUTUS:\n",
      "Well we made no man,\n",
      "You do but we have lend ere be to be doubted.\n",
      "But the marble.\n",
      "\n",
      "CORIOLANUS:\n",
      "Each overthrow will not our brother. He lives\n",
      "We do, that vigour for the hands of the fire--\n",
      "Whose that betrue provided, that they had ever he\n",
      "At until they accused at him again.\n",
      "\n",
      "MENENIUS:\n",
      "O, the south, even so deny these two\n",
      "Left the wivery sovereignty could mean to see;\n",
      "And let them shall delive that die inhire then\n",
      "Which injury you with spring.\n",
      "\n",
      "BRUTUS:\n",
      "'Tis need by them gentle a gentleman.\n",
      "\n",
      "MENENIUS:\n",
      "Now, for valiant.\n",
      "\n",
      "First Senator:\n",
      "I know their own hands.\n",
      "\n",
      "CORIOLANUS:\n",
      "Their strength o'er 'lone.' For slaughter' 'The richly\n",
      "Wanting hath cause upon the full feeding here.'\n",
      "O, it must be a poor much.\n",
      "\n",
      "SICINIUS:\n",
      "That's thy lips: thou'rt they ere a shrude.\n",
      "\n",
      "MENENIUS:\n",
      "Good Camillo, presently.\n",
      "\n",
      "SICINIUS:\n",
      "Why, this' way?\n",
      "Aged Menenius?\n",
      "\n",
      "MENENIUS:\n",
      "Not Romeo come, madam.\n",
      "\n",
      "SICINIUS:\n",
      "He has care, he call him Rome.\n",
      "\n",
      "MENENIUS:\n",
      "This:\n",
      "men'd a slew are unbidding.\n",
      "\n",
      "SICINIUS:\n",
      "Why, then you have moved coming?\n",
      "\n",
      "ESCALUS:\n",
      "I should have thee well avoided the Volsces\n",
      "That which his usurply County.\n",
      "\n",
      "VOLUMNIA:\n",
      "O, sir you\n",
      "Must have been too wife the woman in earth soul;\n",
      "And you in his grace till I leck up the entranchilos.\n",
      "\n",
      "AEdile:\n",
      "We must wife over not?\n",
      "\n",
      "ESCALUS:\n",
      "We must put one.\n",
      "\n",
      "ESCALUS:\n",
      "My pride,--\n",
      "\n",
      "MENENIUS:\n",
      "I will believe thee our fought.\n",
      "\n",
      "ESCALUS:\n",
      "Come on: but I granted them.\n",
      "You have accommanded the further.\n",
      "\n",
      "ESCALUS:\n",
      "Is there made of behave dead?\n",
      "\n",
      "ELBOW:\n",
      "So, they would be hare.\n",
      "Go to: come at your effected title.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BigramLanguageModel().to(device)\n",
    "model.load_state_dict(torch.load(\"bigram_language_model.pt\", map_location=device))\n",
    "model.eval()  # important for dropout/layernorm\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
